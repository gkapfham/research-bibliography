% UNDERGRADUATE THESIS AND DISSERTATION {{{

@phdthesis{Kapfhammer2007d,
  author   = {Gregory M. Kapfhammer},
  title    = {A comprehensive framework for testing database-centric applications},
  school   = {Department of Computer Science, University of Pittsburgh},
  year     = {2007},
  type     = {PhD Dissertation},
  keywords = {kapfhammer},
  abstract = {The database is a critical component of many modern software applications. Recent reports indicate that
              the vast majority of database use occurs from within an application program. Indeed, database-centric
              applications have been implemented to create digital libraries, scientific data repositories, and
              electronic commerce applications. However, a database-centric application is very different from a
              traditional software system because it interacts with a database that has a complex state and structure.
              This dissertation formulates a comprehensive framework to address the challenges that are associated with
              the efficient and effective testing of database-centric applications. The database-aware approach to
              testing includes: (i) a fault model, (ii) several unified representations of a program's database
              interactions, (iii) a family of test adequacy criteria, (iv) a test coverage monitoring component, and (v)
              tools for reducing and re-ordering a test suite during regression testing.<p> This dissertation analyzes
              the worst-case time complexity of every important testing algorithm. This analysis is complemented by
              experiments that measure the efficiency and effectiveness of the database-aware testing techniques. Each
              tool is evaluated by using it to test six database-centric applications. The experiments show that the
              database-aware representations can be constructed with moderate time and space overhead. The adequacy
              criteria call for test suites to cover 20% more requirements than traditional criteria and this ensures
              the accurate assessment of test suite quality. It is possible to enumerate data flow-based test
              requirements in less than one minute and coverage tree path requirements are normally identified in no
              more than ten seconds. The experimental results also indicate that the coverage monitor can insert
              instrumentation probes into all six of the applications in fewer than ten seconds. Although
              instrumentation may moderately increase the static space overhead of an application, the coverage
              monitoring techniques only increase testing time by 55% on average. A coverage tree often can be stored in
              less than five seconds even though the coverage report may consume up to twenty-five megabytes of storage.
              The regression tester usually reduces or prioritizes a test suite in under five seconds. The experiments
              also demonstrate that the modified test suite is frequently more streamlined than the initial tests.},
  presentation = {https://speakerdeck.com/gkapfham/a-comprehensive-framework-for-testing-database-centric-applications},
  presented = {true}
}

@phdthesis{Kapfhammer1999,
  author      = {Gregory M. Kapfhammer},
  title       = {A complex object data generator specification language to facilitate automated data creation in software component test drivers},
  institution = {Department of Computer Science, Allegheny College},
  year        = {1999},
  type        = {Undergraduate Thesis},
  keywords    = {kapfhammer},
  abstract    = {The object-oriented programming (OOP) paradigm provides innovative principles and techniques that
                 enhance the process of software specification, design, and implementation. However, OOP does not
                 provide a silver bullet that will allow engineers to always create software components and systems that
                 attain all of the development goals. In this research we report on the component dependability
                 framework and describe and incremental extension that will allow developers to more easily realize
                 their goals. After an introduction to software testability and component dependability, we address the
                 fundamentals of and current solutions to the automated test data generation problem. By combining
                 different data generation techniques we develop a complex object data generation specification
                 language (CODGSL). This language allows a user to easily create complex component data generators that
                 support arbitrary constraints and complete semantic descriptions. These data generators can be
                 incorporated into to the component test drivers that are produced by the Test Driver Generator (TDoG)
                 in order to automatically provide the needed test data. The integration of the CODGSL and TDoG produces
                 a complete system for designing test drivers that can automatically test any arbitrary component
                 written in Java. The augmented version of TDoG could be used to further automate the calculation of
                 dependability of a Java component and thus provides a useful tool for software developers.},
  nodownload  = {true}
}

% UNDERGRADUATE THESIS AND DISSERTATION }}}

% BOOK CHAPTERS {{{

@incollection{Kapfhammer2010,
  author       = {Gregory M. Kapfhammer},
  title        = {Regression testing},
  booktitle    = {The Encyclopedia of Software Engineering},
  publisher    = {Taylor and Francis -- Auerbach Publications},
  year         = {2010},
  abstract     = {Regression testing techniques execute a test suite whenever the addition of defect fixes or new
                  functionality changes the program under test. The repeated execution of a test suite aims to establish
                  a confidence in the correctness of the software application and identify defects that were introduced
                  by the program modifications. Industry experiences suggest that regression testing often improves the
                  quality of the application under test. However, testing teams may not always perform regression
                  testing because the frequent execution of the tests often incurs high time and space overheads. Test
                  suite selection techniques try to reduce the cost of testing by running a subset of the tests, such as
                  those that execute the modified source code, in order to ensure that the updated program still
                  operates correctly.  Alternatively, reduction methods decrease testing time overheads by discarding
                  the tests that redundantly cover the test requirements. Approaches to test suite prioritization
                  reorder the test cases in an attempt to maximize the rate at which the tests achieve a testing goal
                  such as code coverage. After describing a wide variety of metrics for empirically evaluating different
                  regression testing methods, this chapter considers the efficiency and effectiveness trade-offs
                  associated with these techniques. The conclusion of this article summarizes the state-of-the-art in
                  the field of regression testing and then offers suggestions for future work and resources for further
                  study.}
}

@incollection{Kapfhammer2004,
  author       = {Gregory M. Kapfhammer},
  title        = {Software testing},
  booktitle    = {The Computer Science Handbook},
  publisher    = {CRC Press},
  year         = {2004},
  abstract     = {When a program is implemented to provide a concrete representation of an algorithm, the developers of
                  this program are naturally concerned with the correctness and performance of the implementation.
                  Software engineers must ensure that their software systems achieve an appropriate level of quality.
                  Software verification is the process of ensuring that a program meets its intended specification. One
                  technique that can assist during the specification, design, and implementation of a software system
                  is software verification through correctness proof. Software testing, or the process of assessing the
                  functionality and correctness of a program through execution or analysis, is another alternative for
                  verifying a software system.}
}

% BOOK CHAPTERS }}}

% CONFERENCE PAPERS {{{

@inproceedings{Alsharif2020,
  author       = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title        = {STICCER: Fast and effective database test suite reduction through merging of similar test cases},
  booktitle    = {Proceedings of the 13th International Conference on Software Testing, Verification and Validation},
  year         = {2020},
  abstract     = {Since relational databases support many software applications, industry professionals recommend testing both database
                  queries and the underlying database schema that contains complex integrity constraints. These constraints, which
                  include primary and foreign keys, NOT NULL, and arbitrary CHECK constraints, are important because they protect the
                  consistency and coherency of data in the relational database. Since testing integrity constraints is potentially an
                  arduous task, human testers can use new tools to automatically generate test suites that effectively find schema
                  faults. However, these tool-generated test suites often contain many lengthy tests that may both increase the time
                  overhead of regression testing and limit the ability of human testers to understand them. Aiming to reduce the size of
                  automatically generated test suites for database schemas, this paper introduces STICCER, a technique that finds
                  overlaps between test cases, merging database interactions from similar tests and removing others. By systematically
                  discarding and merging redundant tests, STICCER creates a reduced test suite that is guaranteed to have the same
                  coverage as the original one. Using thirty-four relational database schemas, we experimentally compared STICCER to two
                  greedy test suite reduction techniques and a random method. The results show that, compared to the greedy and random
                  methods, STICCER is the most effective at reducing the number of test cases and database interactions while
                  maintaining test effectiveness as measured by the mutation score.},
  tool         = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/sticcer-fast-and-effective-database-test-suite-reduction-through-merging-of-similar-test-cases},
  presented    = {true}
}

@inproceedings{Alsharif2020a,
  author       = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title        = {Hybrid methods for reducing database schema test suites: Experimental insights from computational and human studies},
  booktitle    = {Proceedings of the 1st International Conference on Automation of Software Test},
  year         = {2020},
  abstract     = {Given that a relational database is a critical component of many software applications, it is
                  important to thoroughly test the integrity constraints of a database's schema, because they protect
                  the data. Although automated test data generation techniques ameliorate the otherwise manual task of
                  database schema testing, they often create test suites that contain many, sometimes redundant, tests.
                  Since prior work presented a hybridized test suite reduction technique, called STICCER, that
                  beneficially combined Greedy test suite reduction with a test merging method customized for database
                  schemas, this paper experimentally evaluates a different hybridization. Motivated by prior results
                  showing that test suite reduction with the Harrold-Gupta-Soffa (HGS) method can be more effective than
                  Greedy at reducing database schema test suites, this paper evaluates an HGS-driven STICCER variant
                  with both a computational and a human study. Using 34 database schemas and tests created by two test
                  data generators, the results from the computational study reveal that, while STICCER is equally
                  efficient and effective when combined with either Greedy or HGS, it is always better than the isolated
                  use of either Greedy or HGS. Involving 27 participants, the human study shows that, when compared to
                  test suites reduced by HGS, those reduced by a STICCER-HGS hybrid allow humans to inspect test cases
                  faster, but not always more accurately.},
  tool         = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/hybrid-methods-for-reducing-database-schema-test-suites-experimental-insights-from-computational-and-human-studies},
  presented    = {true}
}

@inproceedings{Parry2020,
  author       = {Owain Parry and Gregory M. Kapfhammer and Michael Hilton and Phil McMinn},
  title        = {Flake it `till you make it: Using automated repair to induce and fix latent test flakiness},
  booktitle    = {Proceedings of the 1st International Workshop on Automated Program Repair},
  year         = {2020},
  abstract     = {Since flaky tests pass or fail nondeterministically, without any code changes, they are an unreliable
                  indicator of program quality. Developers may quarantine or delete flaky tests because it is often too
                  time consuming to repair them. Yet, since decommissioning too many tests may ultimately degrade a test
                  suite’s effectiveness, developers may eventually want to fix them, a process that is challenging
                  because the nondeterminism may have been introduced previously. We contend that the best time to
                  discover and repair a flaky test is when a developer first creates and best understands it. We refer
                  to tests that are not currently flaky, but that could become so, as having latent flakiness. We
                  further argue that efforts to expose and repair latent flakiness are valuable in ensuring the
                  future-reliability of the test suite, and that the testing cost is greater if latent flakiness is left
                  to manifest itself later. Using concrete examples from a real-world program, this paper posits that
                  automated program repair techniques will prove useful for surfacing latent flakiness.},
}

@inproceedings{Alsharif2019,
  author       = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title        = {What factors make SQL test cases understandable for testers? A human study of automated test data generation techniques},
  booktitle    = {Proceedings of the 35th International Conference on Software Maintenance and Evolution},
  year         = {2019},
  abstract     = {Since relational databases are a key component of software systems ranging from small mobile to large
                  enterprise applications, there are well-studied methods that automatically generate test cases for
                  database-related functionality. Yet, there has been no research to analyze how well testers --- who
                  must often serve as an "oracle" --- both understand tests involving SQL and decide if they reveal
                  flaws. This paper reports on a human study of test comprehension in the context of automatically
                  generated tests that assess the correct specification of the integrity constraints in a relational
                  database schema. In this domain, a tool generates INSERT statements with data values designed to
                  either satisfy (i.e., be accepted into the database) or violate the schema (i.e., be rejected from the
                  database). The study reveals two key findings. First, the choice of data values in INSERTs influences
                  human understandability: the use of default values for elements not involved in the test (but
                  necessary for adhering to SQL's syntax rules) aided participants, allowing them to easily identify and
                  understand the important test values. Yet, negative numbers and "garbage" strings hindered this
                  process. The second finding is more far reaching: humans found the outcome of test cases very
                  difficult to predict when NULL was used in conjunction with foreign keys and CHECK constraints. This
                  suggests that, while including NULL s can surface the confusing semantics of database schemas, their
                  use makes tests less understandable for humans.},
  tool         = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/what-factors-make-sql-test-cases-understandable-for-testers-a-human-study-of-automated-test-data-generation-techniques},
  presented    = {true}
}

@inproceedings{Althomali2019,
  author       = {Ibrahim Althomali and Gregory M. Kapfhammer and Phil McMinn},
  title        = {Automatic visual verification of layout failures in responsively designed web pages},
  booktitle    = {Proceedings of the 12th International Conference on Software Testing, Verification and Validation},
  year         = {2019},
  abstract     = {Responsively designed web pages adjust their layout according to the viewport width of the device in
                  use. Although tools exist to help developers test the layout of a responsive web page, they often rely
                  on humans to flag problems. Yet, the considerable number of web-enabled devices with unique viewport
                  widths makes this manual process both time-consuming and error-prone. Capable of detecting some common
                  responsive layout failures, the ReDeCheck tool partially automates this process. Since ReDeCheck
                  focuses on a web page’s document object model (DOM), some of the issues it finds are not observable by
                  humans. This paper presents a tool, called Viser, that renders a ReDeCheck-reported layout issue in a
                  browser, adjusting the opacity of certain elements and checking for a visible difference. Unless Viser
                  classifies an issue as a human-observable layout failure, a web developer can ignore it. This paper’s
                  experiments reveal the benefit of using Viser to support automated visual verification of layout
                  failures in responsively designed web pages. Viser automatically classified all of the 117 layout
                  failures that ReDeCheck reported for 20 web pages, each of which had to be manually analyzed in a
                  prior study. Viser’s automated manipulation of element opacity also highlighted manual
                  classification’s subjectivity: it categorized 28 issues differently to manual analysis, including
                  three correctly reclassified as false positives.},
  tool         = {https://github.com/redecheck/viser},
  presentation = {https://speakerdeck.com/gkapfham/automatic-visual-verification-of-layout-failures-in-responsively-designed-peb-pages},
  presented    = {true}
}

@inproceedings{Paterson2019,
  author       = {David Paterson and Jos\'{e} Campos and Rui Abreu and Gregory M. Kapfhammer and Gordon Fraser and Phil McMinn},
  title        = {An empirical study on the use of defect prediction for test case prioritization},
  booktitle    = {Proceedings of the 12th International Conference on Software Testing, Verification and Validation},
  year         = {2019},
  abstract     = {Test case prioritization has been extensively researched as a means for reducing the time taken to
                  discover regressions in software. While many different strategies have been developed and evaluated,
                  prior experiments have shown them to not be effective at prioritizing test suites to find real faults.
                  This paper presents a test case prioritization strategy based on defect prediction, a technique that
                  analyzes code features --- such as the number of revisions and authors --- to estimate the likelihood
                  that any given Java class will contain a bug. Intuitively, if defect prediction can accurately predict
                  the class that is most likely to be buggy, a tool can prioritize tests to rapidly detect the defects
                  in that class. We investigated how to configure a defect prediction tool, called Schwa, to maximize
                  the likelihood of an accurate prediction, surfacing the link between perfect defect prediction and
                  test case prioritization effectiveness. Using 6 real-world Java programs containing 395 real faults,
                  we conducted an empirical evaluation comparing this paper’s strategy, called G-clef, against eight
                  existing test case prioritization strategies. The experiments reveal that using defect prediction to
                  prioritize test cases reduces the number of test cases required to find a fault by on average 9.48%
                  when compared with existing coverage-based strategies, and 10.5% when compared with existing
                  history-based strategies.},
  tool         = {https://github.com/kanonizo/kanonizo},
  presentation = {https://speakerdeck.com/gkapfham/an-empirical-study-on-the-use-of-defect-prediction-for-test-case-prioritization},
  presented    = {true}
}

@inproceedings{Alsharif2018,
  author       = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title        = {DOMINO: Fast and effective test data generation for relational database schemas},
  booktitle    = {Proceedings of the 11th International Conference on Software Testing, Verification and Validation},
  year         = {2018},
  abstract     = {An organization's databases are often one of its most valuable assets. Data engineers commonly use a
               relational database because its schema ensures the validity and consistency of the stored data through
               the specification and enforcement of integrity constraints. To ensure their correct specification,
               industry advice recommends the testing of the integrity constraints in a relational schema. Since
               manual schema testing is labor-intensive and error-prone, this paper presents DOMINO, a new
               automated technique that generates test data according to a coverage criterion for integrity
               constraint testing. In contrast to more generalized search-based approaches, which represent the
               current state of the art for this task, DOMINO uses tailored, domain-specific operators to
               efficiently generate test data for relational database schemas. In an empirical study incorporating
               34 relational database schemas hosted by three different database management systems, the results
               show that DOMINO can not only generate test suites faster than the state-of-the-art search-based
               method but that its test suites can also detect more schema faults.},
  data         = {https://github.com/schemaanalyst/domino-replicate},
  tool         = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/domino-fast-and-effective-test-data-generation-for-relational-database-schemas},
  presented    = {true}
}

@inproceedings{Alsharif2018a,
  author    = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title     = {Generating test suites with DOMINO},
  booktitle = {Proceedings of the 11th International Conference on Software Testing, Verification and Validation -- Demonstrations Track},
  year      = {2018},
  abstract  = {Industrial practitioners advocate the testing of relational
               database schemas because, for instance, omitting the definition of an
               integrity constraint (i.e., a PRIMARY KEY or a UNIQUE) in a schema can
               compromise correctness and increase maintenance costs. For example,
               forgetting to mark each username as UNIQUE could lead to incorrect data
               duplication within a database. Also, different database management
               systems (DBMSs) often interpret integrity constraints differently (e.g.,
               a PRIMARY KEY can accept a NULL value once with SQLite but not with
               other DBMSs). In this paper and the accompanying demonstration we introduce
               the use and benefits of DOMINO (DOMain-specific approach to INtegrity cOnstraint
               test data generation), a new technique that automatically generates a test
               suite for a database schema.},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/generating-database-schema-test-suites-with-domino},
  presented = {true}
}

@inproceedings{Alsharif2018b,
  author    = {Abdullah Alsharif and Gregory M. Kapfhammer and Phil McMinn},
  title     = {Running experiments and performing data analysis using SchemaAnalyst and DOMINO},
  booktitle = {Proceedings of the 11th International Conference on Software Testing, Verification and Validation -- Artefacts Track},
  year      = {2018},
  abstract  = {SchemaAnalyst is a tool, developed in the Java programming
               language, that automatically generates tests for complex,
               real-world relational database schemas. It features several data
               generators including DOMINO (DOMain-specific approach to
               INtegrity cOnstraint test data generation), Alternating Variable
               Method (AVM), and Random+. SchemaAnalyst generates tests that
               support three database management systems (DBMSs): PostgreSQL,
               SQLite, and HyperSQL. It also provides a mutation testing tool
               to mutate (i.e., remove, add, or flip) the integrity constraints
               in the schema under test. This paper explains how to run test
               generation experiments and data analysis with SchemaAnalyst and
               its data analysis package written in the R language for
               statistical computation. It will help others to use
               SchemaAnalyst, replicate prior experiments, and conduct new
               studies of schema testing.},
  data      = {https://github.com/schemaanalyst/domino-replicate},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/running-experiments-and-performing-data-analysis-using-schemaanalyst-and-domino},
  presented = {true}
}

@inproceedings{Paterson2018,
  author    = {David Paterson and Gregory M. Kapfhammer and Gordon Fraser and Phil McMinn},
  title     = {Using controlled numbers of real faults and mutants to empirically evaluate coverage-based test case prioritization},
  booktitle = {Proceedings of the 13th International Workshop on Automation of Software Test},
  year      = {2018},
  abstract  = {Used to establish confidence in the correctness of evolving
               software, regression testing is an important, yet costly, task.
               Test case prioritization enables the rapid detection of faults
               during regression testing by reordering the test suite so that
               effective tests are run as early as is possible. However, a
               distinct lack of information about the regression faults found
               in complex real-world software forced prior experimental studies
               of these methods to use artificial faults called mutants. Using
               the Defects4J database of real faults, this paper presents the
               results of experiments evaluating the effectiveness of four
               representative test prioritization techniques. Since this
               paper’s results show that prioritization is susceptible to high
               amounts of variance when only one fault is present, our
               experiments also control the number of real faults and mutants
               in the program subject to regression testing. Our overall
               findings are that, in comparison to mutants, real faults are
               harder for reordered test suites to quickly detect, suggesting
               that mutants are not a surrogate for real faults.},
  tool      = {https://github.com/kanonizo/kanonizo},
  presentation = {https://speakerdeck.com/gkapfham/using-controlled-numbers-of-real-faults-and-mutants-to-empirically-evaluate-coverage-based-test-case-prioritization},
  presented = {true}
}

@inproceedings{Walsh2017,
  author    = {Thomas A. Walsh and Gregory M. Kapfhammer and Phil McMinn},
  title     = {Automated layout failure detection for responsive web pages without an explicit oracle},
  booktitle = {Proceedings of the International Symposium on Software Testing and Analysis},
  year      = {2017},
  abstract  = {As the number and variety of devices being used to access the World Wide Web grows exponentially,
               ensuring the correct presentation of a web page, regardless of the device used to browse it, is an
               important and challenging task. When developers adopt responsive web design (RWD) techniques, web pages
               modify their appearance to accommodate a device's display constraints. However, a current lack of
               automated support means that presentation failures may go undetected in a page's layout when rendered for
               different viewport sizes. A central problem is the difficulty in providing an automated "oracle" to
               validate RWD layouts against, meaning checking for failures is largely a manual process in practice,
               which results in layout failures in many live responsive web sites. This paper presents an automated
               failure detection technique that checks the consistency of a responsive page's layout across a range of
               viewport widths, obviating the need for an explicit oracle. In an empirical study, this method found
               failures in 16 of 26 real-world production pages studied, detecting 33 distinct failures in total.},
  tool      = {https://github.com/redecheck/redecheck},
  presentation = {https://speakerdeck.com/gkapfham/automated-layout-failure-detection-for-responsive-web-pages-without-an-explicit-oracle},
  presented = {true}
}

@inproceedings{Walsh2017a,
  author    = {Thomas A. Walsh and Gregory M. Kapfhammer and Phil McMinn},
  title     = {ReDeCheck: An automatic layout failure checking tool for responsively designed web pages},
  booktitle = {Proceedings of the International Symposium on Software Testing and Analysis},
  year      = {2017},
  abstract  = {Since people frequently access web sites with a wide variety of devices (e.g., mobile phones, laptops,
               and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport
               widths. While responsive web design (RWD) principles and frameworks facilitate the development of such
               sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality
               assurance process for responsively designed web sites is often manual, time-consuming, and error-prone,
               this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential
               unintended regressions in responsive layout and common types of layout failure. In addition to
               summarizing ReDeCheck's benefits, this paper explores two different usage scenarios for this tool that is
               publicly available on GitHub.},
  tool      = {https://github.com/redecheck/redecheck},
  presentation = {https://speakerdeck.com/gkapfham/redecheck-an-automatic-layout-failure-checking-tool-for-responsively-designed-web-pages},
  presented = {true}
}

@inproceedings{Kapfhammer2016,
  author    = {Gregory M. Kapfhammer and Phil McMinn and Chris J. Wright},
  title     = {Hitchhikers need free vehicles! Shared repositories for statistical analysis in {SBST}},
  booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
  year      = {2016},
  abstract  = {As a means for improving the maturity of the data analysis methods used in the search-based software testing
               field, this paper presents the need for shared repositories of well-documented statistical analysis code and
               replication data. In addition to explaining the benefits associated with using these repositories, the paper gives
               suggestions (e.g., the testing of analysis code) for improving the study of data arising from experiments with
               randomized algorithms.},
  paper     = {https://github.com/gkapfham/sbst2016-paper},
  presentation = {https://speakerdeck.com/gkapfham/hitchhikers-need-free-vehicles-shared-repositories-for-statistical-analysis-in-sbst},
  presented = {true}
}

@inproceedings{McCurdy2016,
  author    = {Colton J. McCurdy and Phil McMinn and Gregory M. Kapfhammer},
  title     = {mrstudyr: Retrospectively studying the effectiveness of mutant reduction techniques},
  booktitle = {Proceedings of the 32nd International Conference on Software Maintenance and Evolution},
  year      = {2016},
  abstract  = {Mutation testing is a well-known method for measuring a test suite's quality. However, due to its
               computational expense and intrinsic difficulties (e.g., detecting equivalent mutants and potentially checking a
               mutant's status for each test), mutation testing is often challenging to practically use. To control the
               computational cost of mutation testing, many reduction strategies have been proposed (e.g., uniform random
               sampling over mutants). Yet, a stand-alone tool to compare the efficiency and effectiveness of these methods
               is heretofore unavailable. Since existing mutation testing tools are often complex and language-dependent, this
               paper presents a tool, called <em>mrstudyr</em>, that enables the &quot;retrospective&quot; study of mutant reduction
               methods using the data collected from a prior analysis of all mutants. Focusing on the mutation operators and the
               mutants that they produce, the presented tool allows developers to prototype and evaluate mutant reducers without
               being burdened by the implementation details of mutation testing tools. Along with describing <em>mrstudyr</em>'s
               design and overviewing the experimental results from using it, this paper inaugurates the public release of this
               open-source tool.},
  paper     = {https://github.com/schemaanalyst/icsme2016-mrstudyrtool-paper},
  tool      = {https://github.com/mccurdyc/mrstudyr},
  presentation = {https://speakerdeck.com/gkapfham/mrstudyr-retrospectively-studying-the-effectiveness-of-mutant-reduction-techniques},
  presented = {true}
}

@inproceedings{McMinn2016,
  author    = {Phil McMinn and Mark Harman and Gordon Fraser and Gregory M. Kapfhammer},
  title     = {Automated search for good coverage criteria: Moving from code coverage to fault coverage through search-based software engineering},
  booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
  year      = {2016},
  abstract  = {We propose to use Search-Based Software Engineering to automatically evolve coverage criteria that are
               well correlated with fault revelation, through the use of existing fault databases. We explain how
               problems of bloat and overfitting can be ameliorated in our approach, and show how this new method will
               yield insight into faults --- as well as better guidance for Search-Based Software Testing.},
  presentation = {https://speakerdeck.com/gkapfham/automated-search-for-good-coverage-criteria},
  presented = {true}
}

@inproceedings{McMinn2016a,
  author    = {Phil McMinn and Gregory M. Kapfhammer and Chris J. Wright},
  title     = {Virtual mutation analysis of relational database schemas},
  booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
  year      = {2016},
  abstract  = {Relational databases are a vital component of many modern software applications. Key to the definition
               of the database schema --- which specifies what types of data will be stored in the database and the
               structure in which the data is to be organized --- are integrity constraints. Integrity constraints are
               conditions that protect and preserve the consistency and validity of data in the database, preventing
               data values that violate their rules from being admitted into database tables. They encode logic about
               the application concerned, and like any other component of a software application, need to be properly
               tested. Mutation analysis is a technique that has been successfully applied to integrity constraint
               testing, seeding database schema faults of both omission and commission. Yet, as for traditional
               mutation analysis for program testing, it is costly to perform, since the test suite under analysis
               needs to be run against each individual mutant to establish whether or not it exposes the fault. One
               overhead incurred by database schema mutation is the cost of communicating with the database management
               system (DBMS). In this paper, we seek to eliminate this cost by performing mutation analysis virtually
               on a local model of the DBMS, rather than on an actual, running instance hosting a real database. We
               present an empirical evaluation of our virtual technique revealing that, across all of the studied
               DBMSs and schemas, the virtual method yields an average time saving of 51% over the baseline.},
  paper     = {https://github.com/gkapfham/ast2016-paper},
  data      = {https://github.com/gkapfham/vmutation-replicate},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/virtual-mutation-analysis-of-relational-database-schemas},
  presented = {true}
}

@inproceedings{McMinn2016b,
  author    = {Phil McMinn and Gregory M. Kapfhammer},
  title     = {AVMf: An open-source framework and implementation of the alternating variable method},
  booktitle = {Proceedings of the 8th International Symposium on Search-Based Software Engineering},
  year      = {2016},
  abstract  = {The Alternating Variable Method (AVM) has been shown to be a fast and effective local search technique
               for search-based software engineering. Recent improvements to the AVM have generalized the
               representations it can optimize and have provably reduced its running time. However, until now, there
               has been no general, publicly-available implementation of the AVM incorporating all of these
               developments. We introduce AVMf, an object-oriented Java framework that provides such an
               implementation. AVMf is available from http://avmframework.org for configuration and use in a wide
               variety of projects.},
  paper     = {https://github.com/AVMf/avmframework-paper},
  tool      = {https://github.com/AVMf/avmf},
  presentation = {https://speakerdeck.com/gkapfham/avmf-an-open-source-implementation-of-the-alternating-variable-method},
  presented = {true}
}

@inproceedings{McMinn2016c,
  author    = {Phil McMinn and Chris J. Wright and Cody Kinneer and Colton J. McCurdy and Michael Camara and Gregory M. Kapfhammer},
  title     = {SchemaAnalyst: Search-based test data generation for relational database schemas},
  booktitle = {Proceedings of the 32nd International Conference on Software Maintenance and Evolution},
  year      = {2016},
  abstract  = {Data stored in relational databases plays a vital role in many aspects of society. When this data is
               incorrect, the services that depend on it may be compromised. The database schema is the artefact responsible for
               maintaining the integrity of stored data. Because of its critical function, the proper testing of the database
               schema is a task of great importance. Employing a search-based approach to generate high-quality test data for
               database schemas, <em>SchemaAnalyst</em> is a tool that supports testing this key software component. This
               presented tool is extensible and includes both an evaluation framework for assessing the quality of the generated
               tests and full-featured documentation. In addition to describing the design and implementation of
               <em>SchemaAnalyst</em> and overviewing its efficiency and effectiveness, this paper coincides with the tool's
               public release, thereby enhancing practitioners' ability to test relational database schemas.},
  paper     = {https://github.com/schemaanalyst/icsme2016-satool-paper},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/schemaanalyst-search-based-test-data-generation-for-relational-database-schemas},
  presented = {true}
}

@inproceedings{Kinneer2015,
  author    = {Cody Kinneer and Gregory M. Kapfhammer and Chris J. Wright and Phil McMinn},
  title     = {Automatically evaluating the efficiency of search-based test data generation for relational database schemas},
  booktitle = {Proceedings of the 27th International Conference on Software Engineering and Knowledge Engineering},
  year      = {2015},
  abstract  = {The characterization of an algorithm's worst-case time complexity is useful because it succinctly
               captures how its runtime will grow as the input size becomes arbitrarily large.  However, for certain
               algorithms --- such as those performing search-based test data generation --- a theoretical
               analysis to determine worst-case time complexity is difficult to generalize and thus not often reported
               in the literature.  This paper introduces a framework that empirically determines an algorithm's
               worst-case time complexity by doubling the size of the input and observing the change in runtime.
               Since the relational database is a centerpiece of modern software and the database's schema is
               frequently untested, we apply the doubling technique to the domain of data generation for relational
               database schemas, a field where worst-case time complexities are often unknown. In addition to
               demonstrating the feasibility of suggesting the worst-case runtimes of the chosen algorithms and
               configurations, the results of our study reveal performance trade-offs in testing strategies for
               relational database schemas.},
  paper     = {https://github.com/gkapfham/seke2015-paper},
  tool      = {https://github.com/kinneerc/ExpOse},
  presentation = {https://speakerdeck.com/gkapfham/automatically-evaluating-the-efficiency-of-search-based-test-data-generation-for-relational-database-schemas},
  presented = {true}
}

@inproceedings{Kinneer2015a,
  author    = {Cody Kinneer and Gregory M. Kapfhammer and Chris J. Wright and Phil McMinn},
  title     = {ExpOse: Inferring worst-case time complexity by automatic empirical study},
  booktitle = {Proceedings of the 27th International Conference on Software Engineering and Knowledge Engineering},
  year      = {2015},
  abstract  = {A useful understanding of an algorithm's efficiency, the worst-case time complexity gives an upper
               bound on how an increase in the size of the input, denoted n, increases the execution time of the
               algorithm, or f(n).  This relationship is often expressed in the "big-Oh" notation, where f(n) is
               O(g(n)) means that the time increases by no more than on order of g(n). Since the worst-case complexity
               of an algorithm is evident when n is large, one approach for determining the big-Oh complexity of an
               algorithm is to conduct a doubling experiment with increasingly bigger input sizes. By measuring the
               time needed to run the algorithm on inputs of size n and 2n, the algorithm's order of growth can be
               determined.  This paper introduces expOse, a tool to derive an "EXPerimental big-Oh" for supporting
               "Scalability Evaluation" --- expOse infers an algorithm's big-Oh order of growth by conducting a
               doubling experiment automatically.},
  paper     = {https://github.com/gkapfham/seke2015-tool-paper},
  tool      = {https://github.com/kinneerc/ExpOse},
}

@inproceedings{Walsh2015,
  author    = {Thomas A. Walsh and Phil McMinn and Gregory M. Kapfhammer},
  title     = {Automatic detection of potential layout faults following changes to responsive web pages},
  booktitle = {Proceedings of the 30th International Conference on Automated Software Engineering},
  year      = {2015},
  abstract  = {Due to the exponential increase in the number of mobile devices being used to access the World Wide
               Web, it is crucial that web sites are functional and user-friendly across a wide range of web-enabled
               devices.  This necessity has resulted in the introduction of responsive web design (RWD), which uses
               complex cascading style sheets (CSS) to fluidly modify a web site's appearance depending on the
               viewport width of the device in use. Although existing tools may support the testing of responsive web
               sites, they are time consuming and error-prone to use because they require manual screenshot inspection
               at specified viewport widths. Addressing these concerns, this paper presents a method that can
               automatically detect potential layout faults in responsively designed web sites. To experimentally
               evaluate this approach, we implemented it as a tool, called ReDeCheck, and applied it to 5 real-world
               web sites that vary in both their approach to responsive design and their complexity. The experiments
               reveal that ReDeCheck finds 91% of the inserted layout faults.},
  tool      = {https://github.com/redecheck/redecheck},
  presentation = {https://speakerdeck.com/gkapfham/automatic-detection-of-potential-layout-faults-following-changes-to-responsive-web-pages},
  presented = {true}
}

@inproceedings{Wright2014,
  author    = {Chris J. Wright and Gregory M. Kapfhammer and Phil McMinn},
  title     = {The impact of equivalent, redundant, and quasi mutants on database schema mutation analysis},
  booktitle = {Proceedings of the 14th International Conference on Quality Software},
  year      = {2014},
  abstract  = {Since the relational database is an important component of real-world software and the schema plays a
               major role in ensuring the quality of the database, relational schema testing is essential.  This paper
               presents methods for improving both the efficiency and accuracy of mutation analysis, an established
               method for assessing the quality of test cases for database schemas.  Using a DBMS-independent abstract
               representation, the presented techniques automatically identify and remove mutants that are either
               equivalent to the original schema, redundant with respect to other mutants, or undesirable because they
               are only valid for certain database systems. Applying our techniques for ineffective mutant removal to
               a variety of schemas, many of which are from real-world sources like the U.S. Department of Agriculture
               and the Stack Overflow website, reveals that the presented static analysis of the DBMS-independent
               representation is multiple orders of magnitude faster than a DBMS-specific method. The results also
               show increased mutation scores in 75% of cases, with 44% of those uncovering a mutation-adequate test
               suite. Combining the presented techniques yields mean efficiency improvements of up to 33.7%, with
               averages across schemas of 1.6% and 11.8% for HyperSQL and PostgreSQL, respectively.},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/the-impact-of-equivalent-redundant-and-quasi-mutants-on-database-schema-mutation-analysis},
  presented = {true}

}

@inproceedings{Kotelyanskii2014a,
  author    = {Anton Kotelyanskii and Gregory M. Kapfhammer},
  title     = {Parameter tuning for search-based test-data generation revisited: Support for previous results},
  booktitle = {Proceedings of the 14th International Conference on Quality Software},
  year      = {2014},
  abstract  = {Although search-based test-data generators, like EvoSuite, efficiently and automatically create
               effective JUnit test suites for Java classes, these tools are often difficult to configure. Prior work
               by Arcuri and Fraser revealed that the tuning of EvoSuite with response surface methodology (RSM)
               yielded a configuration of the test data generator that did not outperform the default configuration.
               Following the experimental design and protocol described by Arcuri and Fraser, this paper presents the
               results of a study that lends further support to prior results: like RSM, the EvoSuite configuration
               identified by the well-known Sequential Parameter Optimization Toolbox (SPOT) failed to significantly
               outperform the default settings. Although this result is negative, it furnishes further empirical
               evidence of the challenge associated with tuning a complex search-based test data generator.  Moreover,
               the outcomes of the presented experiments also suggests that EvoSuite's default parameters have been
               set by experts in the field and are thus suitable for use in future experimental studies and industrial
               testing efforts.},
  presentation = {https://speakerdeck.com/gkapfham/parameter-tuning-for-search-based-test-data-generation-revisited},
  presented = {true}
}

@inproceedings{Kapfhammer2013,
  author    = {Gregory M. Kapfhammer and Phil McMinn and Chris J. Wright},
  title     = {Search-based testing of relational schema integrity constraints across multiple database management systems},
  booktitle = {Proceedings of the 6th International Conference on Software Testing, Verification and Validation},
  year      = {2013},
  abstract  = {There has been much attention to testing applications that interact with database management systems,
               and the testing of individual database management systems themselves. However, there has been very
               little work devoted to testing arguably the most important artefact involving an application supported
               by a relational database --- the underlying schema. This paper introduces a search-based technique
               for generating database table data with the intention of exercising the integrity constraints placed on
               table columns. The development of a schema is a process open to flaws like any stage of application
               development. Its cornerstone nature to an application means that defects need to be found early in
               order to prevent knock-on effects to other parts of a project and the spiralling bug-fixing costs that
               may be incurred. Examples of such flaws include incomplete primary keys, incorrect foreign keys, and
               omissions of NOT NULL declarations. Using mutation analysis, this paper presents an empirical study
               evaluating the effectiveness of our proposed technique and comparing it against a popular tool for
               generating table data, DBMonster. With competitive or faster data generation times, our method
               outperforms DBMonster in terms of both constraint coverage and mutation score.},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/search-based-testing-of-relational-schema-integrity-constraints-scross-multiple-database-management-systems},
  presented = {true}
}

@inproceedings{Wright2013,
  author    = {Chris J. Wright and Gregory M. Kapfhammer and Phil McMinn},
  title     = {Efficient mutation analysis of relational database structure using mutant schemata and parallelisation},
  booktitle = {Proceedings of the 8th International Workshop on Mutation Analysis},
  year      = {2013},
  abstract  = {Mutation analysis is an effective way to assess the quality of input values and test oracles. Yet, since
               this technique requires the generation and execution of many mutants, it often incurs a substantial
               computational cost. In the context of program mutation, the use of mutant schemata and parallelisation
               can reduce the costs of mutation analysis. This paper is the first to apply these approaches to the
               mutation analysis of a relational database schema, arguably one of the most important artefacts in a
               database application. Using a representative set of case studies that vary in both their purpose and
               structure, this paper empirically compares an unoptimised method to four database structure mutation
               techniques that intelligently employ both mutant schemata and parallelisation. The results of the
               experimental study highlight the performance trade-offs that depend on the type of database management
               system (DBMS), underscoring the fact that every DBMS does not support all types of efficient mutation
               analysis. However, the experiments also identify a method that yields a one to ten times reduction in the
               cost of mutation analysis for relational schemas hosted by both the Postgres and SQLite DBMSs.},
  tool      = {https://github.com/schemaanalyst/schemaanalyst},
  presentation = {https://speakerdeck.com/gkapfham/efficient-mutation-analysis-of-relational-database-structure-using-mutant-schemata-and-parallelisation},
  presented = {true}
}

@inproceedings{Lin2013,
  author    = {Lin, Chu-Ti and Chen, Cheng-Ding and Tsai, Chang-Shi and Kapfhammer, Gregory M.},
  title     = {History-based test case prioritization with software version awareness},
  booktitle = {Proceedings of the 18th International Conference on Engineering of Complex Computer Systems},
  year      = {2013},
  abstract  = {Test case prioritization techniques schedule the test cases in an order based on some specific criteria so
               that the tests with better fault detection capability are executed at an early position in the regression
               test suite. Many existing test case prioritization approaches are code-based, in which the testing of each
               software version is considered as an independent process. Actually, the test results of the preceding
               software versions may be useful for scheduling the test cases of the later software versions. Some
               researchers have proposed history-based approaches to address this issue, but they assumed that the
               immediately preceding test result provides the same reference value for prioritizing the test cases of the
               successive software version across the entire lifetime of the software development process. Thus, this
               paper describes ongoing research that studies whether the reference value of the immediately preceding
               test results is version-aware and proposes a test case prioritization approach based on our observations.
               The experimental results indicate that, in comparison to existing approaches, the presented one can
               schedule test cases more effectively.},
  presentation = {https://speakerdeck.com/gkapfham/history-based-test-case-prioritization-with-software-version-awareness},
  presented = {true}
}

@inproceedings{Just2012a,
  author    = {Ren\'{e} Just and Gregory M. Kapfhammer and Franz Schweiggert},
  title     = {Do redundant mutants affect the effectiveness and efficiency of mutation analysis?},
  booktitle = {Proceedings of the 7th International Workshop on Mutation Analysis},
  year      = {2012},
  abstract  = {Mutation analysis is an unbiased and powerful method for assessing input values and test oracles.
               However, in comparison to other techniques, such as those that rely on code coverage, it is a
               computationally-expensive and time-consuming method, especially for large software systems. This high
               cost is due, in part, to the fact that many mutation operators generate redundant mutants that may both
               misrepresent the mutation score and increase the runtime of the mutation analysis process. After showing
               how the conditional operator replacement (COR) mutation operator can be defined in a redundant-free
               manner, this paper uses four real-world programs, ranging in size from 3,000 to nearly 40,000 lines of
               code, to show the prevalence of redundant mutants. Focusing on the conditional operator replacement (COR)
               and relational operator replacement (ROR) mutation operators that create 41% of all mutants in the chosen
               programs, the case study reveals that the removal of redundant mutants reduces the runtime of mutation
               analysis by up to 34%. Additional empirical results show that redundant mutants can lead to a mutation
               score that is misleadingly overestimated by as much as 10%. Overall, this paper convincingly demonstrates
               that it is possible to improve the effectiveness and efficiency of a mutation analysis system by
               identifying and removing redundant mutants.},
  presentation = {https://speakerdeck.com/gkapfham/do-redundant-mutants-affect-the-effectiveness-and-efficiency-of-mutation-analysis},
  presented = {true}
}

@inproceedings{Just2012b,
  author    = {Ren\'{e} Just and Gregory M. Kapfhammer and Franz Schweiggert},
  title     = {Using non-redundant mutation operators and test suite prioritization to achieve efficient and scalable mutation analysis},
  booktitle = {Proceedings of the 23rd International Symposium on Software Reliability Engineering},
  year      = {2012},
  abstract  = {Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test
               oracles. However, its application domain is still limited due to the fact that it is a time consuming and
               computationally expensive method, especially when used with large and complex software systems.
               Addressing these challenges, this paper makes several contributions to significantly improve the
               efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a
               reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators.
               The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated
               mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on
               average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation
               analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized
               workflow that exploits the redundancies and runtime differences of test cases to reorder and split the
               corresponding test suite. Using the same ten open-source applications, an empirical study convincingly
               demonstrates that the combination of non- redundant operators and prioritization leveraging information
               about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by
               as much as 65%.},
  presentation = {https://speakerdeck.com/gkapfham/using-non-redundant-mutation-operators-and-test-suite-prioritization-to-achieve-efficient-and-scalable-mutation-analysis},
  presented = {true}
}

@inproceedings{Kauffman2012a,
  author    = {Jonathan Miller Kauffman and Gregory M. Kapfhammer},
  title     = {A framework to support research in and encourage industrial adoption of regression testing techniques},
  booktitle = {Proceedings of the 7th Testing: Academic and Industrial Conference -- Practice and Research Techniques},
  year      = {2012},
  abstract  = {When software developers make changes to a program, it is possible that they will introduce faults into
               previously working parts of the code. As software grows, a regression test suite is run to ensure that
               the old functionality still works as expected. Yet, as the number of test cases increases, it becomes
               more expensive to execute the test suite. Reduction and prioritization techniques enable developers to
               manage large and unwieldy test suites. However, practitioners and researchers do not always use and study
               these methods due, in part, to a lack of availability. In response to this issue, this paper describes an
               already released open-source framework that supports both research and practice in regression testing.
               The sharing of this framework will enable the replication of empirical studies in regression testing and
               encourage faster industrial adoption of these useful, yet rarely used, techniques.},
  tool      = {https://github.com/kauffmj/proteja},
  presentation = {https://speakerdeck.com/gkapfham/a-framework-to-support-research-in-and-encourage-the-industrial-adoption-of-regression-testing-techniques},
  presented = {true}
}

@inproceedings{Kapfhammer2012a,
  author    = {Gregory M. Kapfhammer},
  title     = {Towards a method for reducing the test suites of database applications},
  booktitle = {Poster Compendium of the 5th International Conference on Software Testing, Verification and Validation},
  year      = {2012},
  abstract  = {Database applications are commonly implemented and used in both industry and academia. These complex and
               rapidly evolving applications often have frequent changes in the source code of the program and the state
               and structure of the database. This paper describes and empirically evaluates a test suite reduction
               technique that improves the efficiency of regression testing for database applications by removing
               redundant tests. The experimental results show that the reduced test suites are between 30% and 80%
               smaller than the original test suite, thus enabling a decrease in testing time ranging from 7% to 78%.
               These empirical outcomes suggest that test suite reduction is a viable method for controlling the costs
               associated with testing rapidly-evolving database applications.},
  presented = {true}
}

@inproceedings{Lin2012,
  author    = {Chu-Ti Lin and Kai-Wei Tang and Cheng-Ding Chen and Gregory M. Kapfhammer},
  title     = {Reducing the cost of regression testing by identifying irreplaceable test cases},
  booktitle = {Proceedings of the 6th International Conference on Genetic and Evolutionary Computing},
  year      = {2012},
  abstract  = {Test suite reduction techniques decrease the cost of software testing by removing the redundant test
               cases from the test suite while still producing a reduced set of tests that yields the same level of code
               coverage as the original suite. Most of the existing approaches to reduction aim to decrease the size of
               the test suite. Yet, the difference in the execution cost of the tests is often significant and it may be
               costly to use a test suite consisting of a few long-running test cases. Thus, this paper proposes an
               algorithm, based on the concept of test irreplaceability, which creates a reduced test suite with a
               decreased execution cost. Leveraging widely used benchmark programs, the empirical study shows that, in
               comparison to existing techniques, the presented algorithm is the most effective at reducing the cost of
               running a test suite.},
  presented = {true}
}

@inproceedings{Clark2011,
  author    = {Clark, Sarah R. and Cobb, Jake and Kapfhammer, Gregory M. and Jones, James A. and Harrold, Mary Jean},
  title     = {Localizing SQL faults in database applications},
  booktitle = {Proceedings of the 26th International Conference on Automated Software Engineering},
  year      = {2011},
  abstract  = {This paper presents a new fault-localization technique designed for applications that interact with a
               relational database. The technique uses dynamic information specific to the application's database, such
               as Structured Query Language (SQL) commands, to provide a fault-location diagnosis. By creating
               statement-SQL tuples and calculating their suspiciousness, the presented method lets the developer
               identify the database commands and the program statements likely to cause the failures. The technique
               also calculates suspiciousness for statement-attribute tuples and uses this information to identify SQL
               fragments that are statistically likely to be responsible for the suspiciousness of that SQL command. The
               paper reports the results of two empirical studies. The first study compares existing and database-aware
               fault-localization methods, and reveals the strengths and limitations of prior techniques, while also
               highlighting the effectiveness of the new approach. The second study demonstrates the benefits of using
               database information to improve understanding and reduce manual debugging effort.},
  presentation = {https://speakerdeck.com/gkapfham/localizing-sql-faults-in-database-applications},
  presented = {true}
}

@inproceedings{Kapfhammer2011,
  author    = {Gregory M. Kapfhammer},
  title     = {Empirically evaluating regression testing techniques: Challenges, solutions, and a potential way forward},
  booktitle = {Proceedings of the 1st International Workshop on Regression Testing},
  year      = {2011},
  abstract  = {The published studies of regression testing methods often contain many of the hallmarks of high quality
               empirical research. Beyond features like clear descriptions of the methodology and the visualization and
               statistical analysis of the data sets, certain papers in this field also provide some of the artifacts
               used in and/or produced by the experiments. Yet, the limited industrial adoption of regression testing
               techniques is due in part to a lack of comprehensive empirical evaluations. Moreover, the regression
               testing community has not achieved a level of experimental reproducibility that would fully establish it
               as a science. After identifying the challenges associated with evaluating regression testing methods,
               this paper advocates a way forward involving a mutually beneficial increased sharing of the inputs,
               outputs, and procedures used in experiments.},
  presentation = {https://speakerdeck.com/gkapfham/empirically-evaluating-regression-testing-techniques-challenges-solutions-and-a-potential-way-forward},
  presented = {true}
}

@inproceedings{Just2011a,
  author    = {Just, Ren{\'e} and Kapfhammer, Gregory M. and Schweiggert, Franz},
  title     = {Using conditional mutation to increase the efficiency of mutation analysis},
  booktitle = {Proceedings of the 6th International Workshop on Automation of Software Test},
  year      = {2011},
  abstract  = {Assessing testing strategies and test sets is a crucial part of software testing. Mutation analysis is,
               among other approaches, a suitable technique for this purpose. However, compared with other methods it is
               rather time-consuming and applying mutation analysis to large software systems is still problematic. This
               paper presents a versatile approach, called conditional mutation, which increases the efficiency of
               mutation analysis. This new method significantly reduces the time overhead for generating and executing
               the mutants. Results are reported for eight investigated programs up to 373,000 lines of code and 406,000
               generated mutants. Furthermore, conditional mutation has been integrated into the Java 6 Standard Edition
               compiler. Thus, it is widely applicable and not limited to a certain testing tool or framework.},
  presentation = {https://speakerdeck.com/gkapfham/using-conditional-mutation-to-increase-the-efficiency-of-mutation-analysis},
  presented  = {true}
}

@inproceedings{Jones2011,
  author    = {William F. Jones and Gregory M. Kapfhammer},
  title     = {Ask and you shall receive: Empirically evaluating declarative approaches to finding data in unstructured heaps},
  booktitle = {Proceedings of the 20th International Conference on Software Engineering and Data Engineering},
  year      = {2011},
  abstract  = {This paper reports on experience with the engineering and empirical evaluation of data management
               software that stores objects in collections like the ArrayList or Vector. While many programs may
               retrieve an object from a collection by iteratively evaluating each object according to a set of
               condition(s), this imperative retrieval process becomes more challenging and error-prone as it applies
               many complex criteria to find the matching objects in multiple collections. Query languages for
               unstructured Java virtual machine (JVM) heaps present an alternative to the imperative approach for
               finding the matching objects. Using a benchmarking framework that measures the performance of declarative
               approaches to identifying certain objects in the JVM heap, this paper empirically evaluates two query
               languages, JQL and JoSQL. Both the experiences and the experimental results reveal trade-offs in the
               performance and overall viability of the query languages and the imperative approaches.},
  presentation = {https://speakerdeck.com/gkapfham/ask-and-you-shall-receive-empirically-evaluating-declarative-approaches-to-finding-data-in-unstructured-heaps},
  presented =  {true}
}

@inproceedings{Yalan2011,
  author    = {Liang Yalan and Changhai Nie and Jonathan M. Kauffman and Gregory M. Kapfhammer and Hareton Leung},
  title     = {Empirically identifying the best genetic algorithm for covering array generation},
  booktitle = {Fast Abstract Compendium of the 3rd International Symposium on Search Based Software Engineering},
  year      = {2011},
  abstract  = {With their many interacting parameters, modern software systems are highly configurable. Combinatorial
               testing is a widely used and practical technique that can detect the failures triggered by the parameters
               and their interactions. One of the key challenges in combinatorial testing is covering array generation,
               an often expensive process that is not always amenable to automation with greedy methods. Researchers
               have proposed many techniques to generate covering arrays. As one of the evolutionary search methods, the
               genetic algorithm often has been effectively applied to solve many complex optimization problems in this
               and other fields. However, the performance of a genetic algorithm is not always stable and thus
               significantly impacted by its configurable parameters. Previous studies have not considered either the
               exploration of the genetic algorithm's optimal configuration or ways to improve its performance for
               covering array generation. In order to close this knowledge gap, we designed three classes of experiments
               (i.e., a pair-wise, base choice, and hill climbing experiment) to systemically examine the impacts of and
               interactions between the genetic algorithm's five configurable parameters (i.e., population size, number
               of generations, crossover probability, mutation probability, and genetic algorithm variant). Overall, the
               goal of this paper is to answer the following two questions: (1) Is there an improved configuration of a
               genetic algorithm for a particular pair-wise SUT? and (2) Is there a common improved configuration for
               all pair-wise SUTs?},
  presentation = {https://speakerdeck.com/gkapfham/empirically-identifying-the-best-genetic-algorithm-for-covering-array-generation},
  presented  = {true}
}

@inproceedings{Cobb2011,
  author    = {Cobb, Jake and Jones, James A. and Kapfhammer, Gregory M. and Harrold, Mary Jean},
  title     = {Dynamic invariant detection for relational databases},
  booktitle = {Proceedings of the 9th International Workshop on Dynamic Analysis},
  year      = {2011},
  abstract  = {Despite the many automated techniques that benefit from dynamic invariant detection, to date, none are
               able to capture and detect dynamic invariants at the interface of a program and its databases. This paper
               presents a dynamic invariant detection method for relational databases and for programs that use
               relational databases and an implementation of the approach that leverages the Daikon dynamic-invariant
               engine. The method defines a mapping between relational database elements and Daikon's notion of program
               points and variable observations, thus enabling row-level and column-level invariant detection. The paper
               also presents the results of two empirical evaluations on four fixed data sets and three subject
               programs. The first study shows that dynamically detecting and inferring invariants in a relational
               database is feasible and 55% of the invariants produced for each subject are meaningful. The second study
               reveals that all of these meaningful invariants are schema-enforceable using standards-compliant
               databases and many can be checked by databases with only limited schema constructs.},
  presentation = {https://speakerdeck.com/gkapfham/dynamic-invariant-detection-for-relational-databases},
  presented = {true}
}

@inproceedings{Just2011b,
  author    = {Just, Ren{\'e} and Kapfhammer, Gregory M. and Schweiggert, Franz},
  title     = {MAJOR: An efficient and extensible tool for mutation analysis in a Java compiler},
  booktitle = {Proceedings of the 26th International Conference on Automated Software Engineering},
  year      = {2011},
  abstract  = {Mutation analysis is an effective, yet often time-consuming and difficult-to-use method for the
               evaluation of testing strategies. In response to these and other challenges, this paper presents MAJOR, a
               fault seeding and mutation analysis tool that is integrated into the Java Standard Edition compiler as a
               non-invasive enhancement for use in any Java-based development environment. MAJOR reduces the mutant
               generation time and enables efficient mutation analysis. It has already been successfully applied to
               large applications with up to 373,000 lines of code and 406,000 mutants. Moreover, MAJOR's domain
               specific language for specifying and adapting mutation operators also makes it extensible. Due to its
               ease-of-use, efficiency, and extensibility, MAJOR is an ideal platform for the study and application of
               mutation analysis.},
  presentation = {https://speakerdeck.com/gkapfham/major-an-efficient-and-extensible-tool-for-mutation-analysis-in-a-java-compiler},
  presented = {true}
}

@inproceedings{Conrad2010a,
  author    = {Conrad, Alexander P. and Roos, Robert S. and Kapfhammer, Gregory M.},
  title     = {Empirically studying the role of selection operators during search-based test suite prioritization},
  booktitle = {Proceedings of the 12th International Conference on Genetic and Evolutionary Computation},
  year      = {2010},
  keywords  = {award},
  abstract  = {Regression test suite prioritization techniques reorder test cases so that, on average, more faults will
               be revealed earlier in the test suite's execution than would otherwise be possible. This paper presents a
               genetic algorithm-based test prioritization method that employs a wide variety of mutation, crossover,
               selection, and transformation operators to reorder a test suite. Leveraging statistical analysis
               techniques, such as tree model construction through binary recursive partitioning and kernel density
               estimation, the paper's empirical results highlight the unique role that the selection operators play in
               identifying an effective ordering of a test suite. The study also reveals that, while truncation
               selection consistently outperformed the tournament and roulette operators in terms of test suite
               effectiveness, increasing selection pressure consistently produces the best results within each class of
               operator. After further explicating the relationship between selection intensity, termination condition,
               fitness landscape, and the quality of the resulting test suite, this paper demonstrates that the genetic
               algorithm-based prioritizer is superior to random search and hill climbing and thus suitable for many
               regression testing environments.},
  tool       = {https://github.com/gkapfham/gelations},
  presentation = {https://speakerdeck.com/gkapfham/empirically-studying-the-role-of-selection-operators-during-search-based-test-suite-prioritization},
  presented  = {true}
}

@inproceedings{Kukunas2010a,
  author    = {Kukunas, James and Cupper, Robert D. and Kapfhammer, Gregory M.},
  title     = {A genetic algorithm to improve Linux kernel performance on resource-constrained devices},
  booktitle = {Proceedings of the 12th International Conference Companion on Genetic and Evolutionary Computation},
  year      = {2010},
  abstract  = {As computers become increasingly mobile, users demand more functionality, longer battery-life, and better
               performance from mobile devices. In response, chipset fabricators are focusing on elegant architectures
               to provide solutions that are both low-power and high-performance. Since these architectures rely on
               unique x86 extensions rather than fast clock speeds and large caches, careful thought must be placed into
               effective optimization strategies for not only user applications, but also the kernel itself, as the
               typical default optimizations used by modern compilers do not often take advantage of these specialized
               features. Focusing on the Intel Diamondville platform, this paper presents a genetic algorithm that
               evolves the compiler flags needed to build a Linux kernel that exhibits reduced response times.},
  presentation = {https://speakerdeck.com/gkapfham/a-genetic-algorithm-to-improve-linux-kernel-performance-on-resource-constrained-devices},
  presented  = {true}
}

@inproceedings{Williams2010,
  author    = {Zachary Williams and Gregory M. Kapfhammer},
  title     = {Using synthetic test suites to empirically compare search-based and greedy prioritizers},
  booktitle = {Proceedings of the 12th International Conference Companion on Genetic and Evolutionary Computation},
  year      = {2010},
  abstract  = {The increase in the complexity of modern software has led to the commensurate growth in the size and
               execution time of the test suites for these programs. In order to address this alarming trend, developers
               use test suite prioritization to reorder the test cases so that faults can be detected at an early stage
               of testing. Yet, the implementation and evaluation of greedy and search-based test prioritizers requires
               access to case study applications and their associated test suites, which are often difficult to find,
               configure, and use in an empirical study. This paper presents two types of synthetically generated test
               suites that support this process of experimentally evaluating prioritization methods. Using synthetic
               test suites affords greater control over test case characteristics and supports the identification of
               empirical trends that contradict the established wisdom about search-based and greedy prioritization. For
               instance, we find that the hill climbing algorithm often exhibits a lower time overhead than the greedy
               test suite prioritizer while producing test orderings with comparable effectiveness scores.},
  presentation = {https://speakerdeck.com/gkapfham/using-synthetic-test-suites-to-empirically-compare-search-based-and-greedy-prioritizers},
  presented  = {true}
}

@inproceedings{Smith2009,
  author    = {Smith, Adam M. and Kapfhammer, Gregory M.},
  title     = {An empirical study of incorporating cost into test suite reduction and prioritization},
  booktitle = {Proceedings of the 24th Symposium on Applied Computing},
  year      = {2009},
  abstract  = {Software developers use testing to gain and maintain confidence in the correctness of a software system.
               Automated reduction and prioritization techniques attempt to decrease the time required to detect faults
               during test suite execution. This paper uses the Harrold Gupta Soffa, delayed greedy, traditional greedy,
               and 2-optimal greedy algorithms for both test suite reduction and prioritization. Even though reducing
               and reordering a test suite is primarily done to ensure that testing is cost-effective, these algorithms
               are normally configured to make greedy choices with coverage information alone. This paper extends these
               algorithms to greedily reduce and prioritize the tests by using both test cost (e.g., execution time) and
               the ratio of code coverage to test cost. An empirical study with eight real world case study applications
               shows that the ratio greedy choice metric aids a test suite reduction method in identifying a smaller and
               faster test suite. The results also suggest that incorporating test cost during prioritization allows for
               an average increase of 17% and a maximum improvement of 141% for a time sensitive evaluation metric
               called coverage effectiveness.},
  tool       = {https://github.com/gkapfham/raise},
  presentation = {https://speakerdeck.com/gkapfham/an-empirical-study-of-incorporating-cost-into-test-suite-reduction-and-prioritization},
  presented  = {true}
}

@inproceedings{Bhadra2009,
  author    = {Suvarshi Bhadra and Alexander P. Conrad and Charles P. Hurkes and Brian Kirklin and Gregory M. Kapfhammer},
  title     = {An experimental study of methods for executing test suites in memory constrained environments},
  booktitle = {Proceedings of the 4th International Workshop on the Automation of Software Test},
  year      = {2009},
  abstract  = {Software for memory constrained mobile devices is often implemented in the Java programming language
               because the Java compiler and virtual machine (JVM) provide enhanced safety, portability, and the
               potential for run-time optimization. However, testing time may increase substantially when memory is
               limited and the JVM employs a compiler to create native code bodies. This paper furnishes an empirical
               study that identifies the fundamental trade-offs associated with a method that uses adaptive native code
               unloading to perform memory constrained testing. The experimental results demonstrate that code unloading
               can reduce testing time by 17% and the code size of the test suite and application under test by 68%
               while maintaining the overall size of the JVM. We also find that the goal of reducing the space overhead
               of an automated testing technique is often at odds with the objective of decreasing the time required to
               test. Additional experiments reveal that using a complete record of test suite behavior, in contrast to a
               sample-based profile, does not enable the code unloader to make decisions that markedly reduce testing
               time. Finally, we identify test suite and application behaviors that may limit the effectiveness of our
               method for memory constrained test execution and we suggest ways to mitigate these challenges.},
  presentation = {https://speakerdeck.com/gkapfham/an-experimental-study-of-methods-for-executing-test-suites-in-memory-constrained-environments},
  presented  = {true}
}

@inproceedings{Smith2009a,
  author    = {Adam M. Smith and Joshua J. Geiger and Gregory M. Kapfhammer and Manos Renieris and G. Elisabeta Marai},
  title     = {Interactive coverage effectiveness multiplots for evaluating prioritized regression test suites},
  booktitle = {Compendium of the 15th Information Visualization Conference},
  year      = {2009},
  abstract  = {Software testing increases confidence in the correctness of an application's source code. Altering a test
               suite's execution order enables earlier detection of defects and allows developers to fix errors sooner.
               The many existing prioritization methods produce different possible test suite orders from which to
               choose. We propose, implement in a free and open source system, and informally evaluate Interactive
               Coverage Effectiveness Multiplots, an interactive visualization technique that allows software testers to
               quickly filter, appraise and compare the effectiveness of many test suite orders. Preliminary results
               show that researchers, students, and practitioners in the field of software testing find the system
               useful.},
  tool       = {https://github.com/gkapfham/raise},
  presented  = {true}
}

@inproceedings{Ostrofsky2009,
  author    = {Erik Ostrofsky and Gregory M. Kapfhammer},
  booktitle = {Poster Compendium of the 3rd International Symposium on Empirical Software Engineering and Measurement},
  title     = {An empirical comparison of methods for compressing test coverage reports},
  year      = {2009},
  abstract  = {Test coverage monitoring techniques are an integral part of modern methodologies for testing computer
               software. For instance, tools such as automated fault localizers, test adequacy calculators, and
               debuggers all use a coverage report for various purposes. Recently developed monitoring methods track the
               coverage of the nodes and edges in a program's control flow graph, definition-use associations involving
               program variables, or interaction with the state and structure of a database. However, a coverage report
               often balloons in size as the monitor includes additional details about the behavior of the program, test
               suite, and other software components such as the operating system and database. The marked increase in
               coverage report size is particularly problematic when testing occurs in a resource constrained embedded
               environment or on a build/test server that collects coverage results for many programs over a long time
               period. Large coverage reports may also limit the efficiency and effectiveness of defect isolation
               methods that monitor a remote program and thus transmit coverage data across a network.},
  presented  = {true}
}

@inproceedings{Kapfhammer2008,
  author    = {Gregory M. Kapfhammer and Mary Lou Soffa},
  title     = {Database-aware test coverage monitoring},
  booktitle = {Proceedings of the 1st India Software Engineering Conference},
  year      = {2008},
  abstract  = {Unlike traditional programs, a database-centric application interacts with a database that has a complex
               state and structure. Even though the database is an important component of modern software, there are few
               tools to support the testing of database-centric applications. This paper presents a test coverage
               monitoring technique that tracks a program's definition and use of database entities during test suite
               execution. The paper also describes instrumentation probes that construct a coverage tree that records
               how the program and the tests cover the database. We conducted experiments to measure the costs that are
               associated with (i) instrumenting the program and the tests and (ii) monitoring coverage. For all of the
               applications, the experiments demonstrate that the instrumentation mechanism incurs an acceptable time
               overhead. While the use of statically inserted probes may increase the size of an application, this
               approach enables database-aware coverage monitoring that increases testing time from 13% to no more than
               54%.},
  presentation = {https://speakerdeck.com/gkapfham/database-aware-test-coverage-monitoring},
  presented =  {true}
}

@inproceedings{Bhadra2008a,
  author    = {Suvarshi Bhadra and Gregory M. Kapfhammer},
  title     = {Prioritizing test suites by finding Hamiltonian paths: Preliminary studies and initial results},
  booktitle = {Fast Abstract Compendium of the 7th Testing: Academic and Industrial Conference -- Practice and Research Techniques},
  year      = {2008},
  abstract  = {This paper describes a technique for prioritizing a test suite by finding the least weight Hamiltonian
               path in a complete graph that represents relative testing costs. Our technique is especially useful when
               testing confronts constraints such as quotas in a Web service, memory overhead, or test execution time.
               During the testing of modern mobile computing devices (e.g., handsets running Google Android), it is
               often challenging to properly handle memory constraints. Thus, even though we anticipate that our
               approach is valuable in a wide variety of limited resource environments, this paper focuses on
               prioritizing test suites for memory constrained execution.},
  presentation = {https://speakerdeck.com/gkapfham/prioritizing-test-suites-by-finding-hamiltonian-paths-preliminary-studies-and-initial-results},
  presented  = {true}
}

@inproceedings{Smith2007b,
  author    = {Adam M. Smith and Joshua J. Geiger and Gregory M. Kapfhammer and Mary Lou Soffa},
  title     = {Test suite reduction and prioritization with call trees},
  booktitle = {Proceedings of the 22nd International Conference on Automated Software Engineering},
  year      = {2007},
  abstract  = {This paper presents a tool that (i) constructs tree-based models of a program's behavior during testing
               and (ii) employs these trees while reordering and reducing a test suite. Using either a dynamic call tree
               or a calling context tree, the test reduction component identifies a subset of the original tests that
               covers the same call tree paths. The prioritization technique reorders a test suite so that it covers the
               call tree paths more rapidly than the initial test ordering. In support of program and test suite
               understanding, the tool also visualizes the call trees and the coverage relationships. For a chosen case
               study application, the experimental results show that call tree construction only increases testing time
               by 13%. In comparison to the original test suite, the experiments show that (i) a prioritized suite
               achieves coverage much faster and (ii) a reduced test suite contains 45% fewer tests and consumes 82%
               less time.},
  presentation = {https://speakerdeck.com/gkapfham/test-suite-reduction-and-prioritization-with-call-trees},
  presented  = {true}
}

@inproceedings{Kapfhammer2007,
  author    = {Gregory M. Kapfhammer and Mary Lou Soffa},
  title     = {Using coverage effectiveness to evaluate test suite prioritizations},
  booktitle = {Proceedings of the International Workshop on Empirical Assessment of Software Engineering Languages and Technologies},
  year      = {2007},
  abstract  = {Regression test suite prioritization techniques reorder a test suite with the goal of ensuring that the
               reorganized test suite finds faults faster than the initial ordering. It is challenging to empirically
               evaluate the effectiveness of a new test case arrangement because existing metrics (i) require fault
               seeding or (ii) ignore test case costs. This paper presents a coverage effectiveness (CE) metric that (i)
               obviates the need to seed faults into the program under test and (ii) incorporates available data about
               test case execution times. A test suite is awarded a high CE value when it quickly covers the test
               requirements. It is possible to calculate coverage effectiveness regardless of the coverage criterion
               that is chosen to evaluate test case quality. The availability of an open source CE calculator enables
               future case studies and controlled experiments to use coverage effectiveness when evaluating different
               approaches to test suite prioritization.}
}

@inproceedings{Alspaugh2007,
  author    = {Sara Alspaugh and Kristen R. Walcott and Michael Belanich and Gregory M. Kapfhammer and Mary Lou Soffa},
  title     = {Efficient time-aware prioritization with knapsack solvers},
  booktitle = {Proceedings of the International Workshop on Empirical Assessment of Software Engineering Languages and Technologies},
  year      = {2007},
  abstract  = {Regression testing is frequently performed in a time constrained environment. This paper explains how 0/1
               knapsack solvers (e.g., greedy, dynamic programming, and the core algorithm) can identify a test suite
               reordering that rapidly covers the test requirements and always terminates within a specified testing
               time limit. We conducted experiments that reveal fundamental trade-offs in the (i) time and space costs
               that are associated with creating a reordered test suite and (ii) quality of the resulting
               prioritization. We find knapsack-based prioritizers that ignore the overlap in test case coverage incur a
               low time overhead and a moderate to high space overhead while creating prioritizations exhibiting a minor
               to modest decrease in effectiveness. We also find that the most sophisticated 0/1 knapsack solvers do not
               always identify the most effective prioritization, suggesting that overlap-aware prioritizers with a
               higher time overhead are useful in certain testing contexts.}
}

@inproceedings{Walcott2006,
  author    = {Kristen R. Walcott and Mary Lou Soffa and Gregory M. Kapfhammer and Robert S. Roos},
  title     = {Time-aware test suite prioritization},
  booktitle = {Proceedings of the International Symposium on Software Testing and Analysis},
  year      = {2006},
  abstract  = {Regression testing is frequently performed in a time constrained environment. This paper explains how 0/1
               knapsack solvers (e.g., greedy, dynamic programming, and the core algorithm) can identify a test suite
               reordering that rapidly covers the test requirements and always terminates within a specified testing
               time limit. We conducted experiments that reveal fundamental trade-offs in the (i) time and space costs
               that are associated with creating a reordered test suite and (ii) quality of the resulting
               prioritization. We find knapsack-based prioritizers that ignore the overlap in test case coverage incur a
               low time overhead and a moderate to high space overhead while creating prioritizations exhibiting a minor
               to modest decrease in effectiveness. We also find that the most sophisticated 0/1 knapsack solvers do not
               always identify the most effective prioritization, suggesting that overlap-aware prioritizers with a
               higher time overhead are useful in certain testing contexts.},
  presentation = {https://speakerdeck.com/gkapfham/time-aware-test-suite-prioritization},
  presented  = {true}
}

@inproceedings{Rummel2005a,
  author    = {Matthew J. Rummel and Gregory M. Kapfhammer and Andrew Thall},
  title     = {Towards the prioritization of regression test suites with data flow information},
  booktitle = {Proceedings of the 20th Symposium on Applied Computing},
  year      = {2005},
  abstract  = {Regression test prioritization techniques re-order the execution of a test suite in an attempt to ensure
               that defects are revealed earlier in the test execution phase. In prior work, test suites were
               prioritized with respect to their ability to satisfy control flow-based and mutation-based test adequacy
               criteria. In this paper, we propose an approach to regression test prioritization that leverages the
               all-DUs test adequacy criterion that focuses on the definition and use of variables within the program
               under test. Our prioritization scheme is motivated by empirical studies that have shown that (i) tests
               fulfilling the all-DUs test adequacy criteria are more likely to reveal defects than those that meet the
               control flow-based criteria, (ii) there is an unclear relationship between all-DUs and mutation-based
               criteria, and (iii) mutation-based testing is significantly more expensive than testing that relies upon
               all-DUs. <p> In support of our prioritization technique, we provide a formal statement of the algorithms
               and equations that we use to instrument the program under test, perform test suite coverage monitoring,
               and calculate test adequacy. Furthermore, we examine the architecture of a tool that implements our novel
               prioritization scheme and facilitates experimentation. The use of this tool in a preliminary experimental
               evaluation indicates that, for three case study applications, our prioritization can be performed with
               acceptable time and space overheads. Finally, these experiments also demonstrate that the prioritized
               test suite can have an improved potential to identify defects earlier during the process of test
               execution.},
  presentation = {https://speakerdeck.com/gkapfham/towards-the-prioritization-of-regression-test-suites-with-data-flow-information},
  presented  = {true}
}

@inproceedings{Kapfhammer2005,
  author    = {Gregory M. Kapfhammer and Mary Lou Soffa and Daniel Moss\'{e}},
  title     = {Testing in resource-constrained execution environments},
  booktitle = {Proceedings of the 20th International Conference on Automated Software Engineering},
  year      = {2005},
  abstract  = {Resource constrained embedded devices are becoming increasingly popular and affordable. Software for
               these devices is often implemented in the Java programming language because the Java compiler and virtual
               machine provide enhanced safety, portability, and the potential for run-time optimization. It is
               important to verify that a software application executes correctly in the environment in which it will
               normally execute, even if this environment is an embedded one that severely constrains memory resources.
               Testing can be used to isolate defects within and establish a confidence in the correctness of a Java
               application that executes in a resource constrained environment. However, executing test suites with a
               Java virtual machine that uses dynamic compilation to create native code bodies can create significant
               testing time overheads if memory resources are highly constrained. This paper describes an approach that
               uses adaptive code unloading to ensure that it is feasible to perform testing in the actual memory
               constrained execution environment. Our experiments demonstrate that code unloading can reduce both the
               test suite execution time by 34% and the code size of the test suite and application under test by 78%
               while maintaining the overall size of the Java virtual machine.},
  presentation = {https://speakerdeck.com/gkapfham/testing-in-resource-constrained-execution-environments},
  presented  = {true}
}

@inproceedings{Howell2003a,
  author    = {Christopher J. Howell and Gregory M. Kapfhammer and Robert S. Roos},
  title     = {An examination of the run-time performance of GUI creation frameworks},
  booktitle = {Proceedings of the 2nd International Conference on the Principles and Practice of Programming in Java},
  year      = {2003},
  abstract  = {The graphical user interface (GUI) is an important component of many software systems. Past surveys
               indicate that the development of a GUI is a significant undertaking and that the GUI's source code often
               comprises a substantial portion of the program's overall source base. Graphical user interface creation
               frameworks for popular object-oriented programming languages enable the rapid construction of simple and
               complex GUIs. In this paper, we examine the run-time performance of two GUI creation frameworks, Swing
               and Thinlet, that are tailored for the Java programming language. Using a simple model of a Java GUI, we
               formally define the difficulty of a GUI manipulation event. After implementing a case study application,
               we conducted experiments to measure the event handling latency for GUI manipulation events of varying
               difficulties. During our investigation of the run-time performance of the Swing and Thinlet GUI creation
               frameworks, we also measured the CPU and memory consumption of our candidate application during the
               selected GUI manipulation events. Our experimental results indicate that Thinlet often outperformed Swing
               in terms of both event handling latency and memory consumption. However, Swing appears to be better
               suited, in terms of event handling latency and CPU consumption, for the construction of GUIs that require
               manipulations of high difficulty levels.},
  presentation = {https://speakerdeck.com/gkapfham/an-examination-of-the-run-time-performance-of-gui-creation-frameworks},
  presented  = {true}
}

@inproceedings{Kapfhammer2003,
  author    = {Gregory M. Kapfhammer and Mary Lou Soffa},
  title     = {A family of test adequacy criteria for database-driven applications},
  booktitle = {Proceedings of the 9th European Software Engineering Conference and the 11th Symposium on the Foundations of Software Engineering},
  year      = {2003},
  abstract  = {Although a software application always executes within a particular environment, current testing methods
               have largely ignored these environmental factors. Many applications execute in an environment that
               contains a database. In this paper, we propose a family of test adequacy criteria that can be used to
               assess the quality of test suites for database-driven applications. Our test adequacy criteria use
               dataflow information that is associated with the entities in a relational database. Furthermore, we
               develop a unique representation of a database-driven application that facilitates the enumeration of
               database interaction associations. These associations can reflect an application's definition and use of
               database entities at multiple levels of granularity. The usage of a tool to calculate intraprocedural
               database interaction associations for two case study applications indicates that our adequacy criteria
               can be computed with an acceptable time and space overhead.},
  presentation = {https://speakerdeck.com/gkapfham/a-family-of-test-adequacy-criteria-for-database-driven-applications},
  presented  = {true}
}

@inproceedings{Haddox2002,
  author    = {Jennifer Haddox and Gregory M. Kapfhammer and C.C. Michael},
  title     = {An approach for understanding and testing third-party software components},
  booktitle = {Proceedings of the 48th Reliability and Maintainability Symposium},
  year      = {2002},
  abstract  = {In this paper, we present an approach to mitigating software risk by understanding and testing third
               party, or commercial-off-the-shelf (COTS), software components. Our approach, based on the notion of
               software wrapping, gives system integrators an improved understanding of how a COTS component behaves
               within a particular system. Our approach to wrapping allows the data flowing into and out of the
               component at the public interface level to be intercepted. Using our wrapping approach, developers can
               apply testing techniques such as fault injection, data collection and assertion checking to components
               whose source code is unavailable. <p> We have created a methodology for using software wrapping in
               conjunction with data collection, fault injection, and assertion checking to test the interaction between
               a component and the rest of the application. The methodology seeks to identify locations in the program
               where the system's interaction with COTS components could be problematic. Furthermore, we have developed
               a prototype that implements our methodology for Java applications. The goal of this process is to allow
               the developers to identify scenarios where the interaction between COTS software and the system could
               result in system failure. We believe that the technology we have developed is an important step towards
               easing the process of using COTS components in the building and maintenance of software systems.}
}

@inproceedings{Arnold2002a,
  author    = {Geoffrey C. Arnold and Gregory M. Kapfhammer and Robert S. Roos},
  title     = {Implementation and analysis of a JavaSpace supported by a relational database},
  booktitle = {Proceedings of the 8th International Conference on Parallel and Distributed Processing Techniques and Applications},
  year      = {2002},
  abstract  = {The combination of the Jini network technology and the JavaSpaces object repository provides an
               exceptional environment for the design and implementation of loosely coupled distributed systems. We
               explore the strengths and weaknesses of the most popular implementation of a persistent JavaSpace. We
               report on the design, implementation, and analysis of RDBSpace, a JavaSpace that is supported by a
               relational database. Our experimental results indicate that it is possible to build an efficient and
               scalable JavaSpace that relies on a relational database back-end.}
}

@inproceedings{Zorman2002a,
  author    = {Brian Zorman and Gregory M. Kapfhammer and Robert S. Roos},
  title     = {Creation and analysis of a JavaSpace-based genetic algorithm},
  booktitle = {Proceedings of the 8th International Conference on Parallel and Distributed Processing Techniques and Applications},
  year      = {2002},
  abstract  = {The island model for distributed genetic algorithms (GAs) is a natural match for the master-worker
               paradigm in distributed computation. We explore the benefits and drawbacks of several distributed system
               architectures in developing an implementation of a distributed GA that exploits the Jini and JavaSpace
               technologies. Our results, using the knapsack problem as an illustration, show that there is an
               unavoidable price to pay in terms of decreasing computation-to-communication ratios as a function of
               instance size. However, we can diminish these effects by expanding the number of JavaSpaces beyond those
               required for the obvious implementation. Our results also indicate that as the number of remote machines
               increases the potential for a better solution also rises. Even though our distributed GAs did not always
               exploit this potential for a higher quality solution, we believe that the combination of Java, Jini, and
               JavaSpaces presents avenues for easily distributing the computation of genetic algorithms.}
}

@inproceedings{Bittman2001a,
  author    = {Marcus Bittman and Robert S. Roos and Gregory M. Kapfhammer},
  title     = {Creating a free, dependable software engineering environment for building Java applications},
  booktitle = {Proceedings of the 1st International Workshop on Open Source Software Engineering},
  year      = {2001},
  abstract  = {As open source software engineering becomes more prevalent, employing sound software engineering
               practices and the tools used to implement these practices becomes more important. This paper examines the
               current status of free software engineering tools. For each set of tools, we determined the important
               attributes that would best assist a developer in each stage of the waterfall model. We rated each tool
               based on predetermined attributes. We used the creation of a graphical user interface based email client
               in Java to assist in evaluating each tool. Our findings show that there is still a need for free tools to
               extract UML diagrams, test graphical user interfaces, make configuring Emacs easier, and profile Java
               applications. In other areas there are free tools that provide satisfactory functionality such as
               Concurrent Versions System (CVS), GVim, JUnit, JRefactory, GNU Make, Jakarta Ant, Javadoc, and Doc++.}
}

@inproceedings{Haddox2001,
  author     = {Jennifer Haddox and Gregory M. Kapfhammer and C.C. Michael and Michael Schatz},
  title      = {Testing commercial-off-the-shelf components with software wrappers},
  booktitle  = {Proceedings of the 18th International Conference on Testing Computer Software},
  year       = {2001},
  abstract   = {Today, software developers do not rely solely on custom built software systems to construct large scale
               information systems. The system being built and maintained are made up of a combination of
               commercial-off-the-shelf (COTS) components, legacy software, and custom built components. Corporate
               down-sizing and decrease government budgets, as well as the spiraling costs of building and maintaining
               large software systems, have made the development of a system completely from original components and
               impossibility. A result of this transition has been that developers are relying on the usage of
               commercial-off-the-shelf components with increased frequency.},
  nodownload = {true}
}

@inproceedings{Kapfhammer2001,
  author    = {Gregory M. Kapfhammer},
  title     = {Automatically and transparently distributing the execution of regression test suites},
  booktitle = {Proceedings of the 18th International Conference on Testing Computer Software},
  year      = {2001},
  abstract  = {From the perspective of the software testing practitioner, regression testing is often an important
               technique for developing high quality software systems. The software testing research community has
               developed several different approaches in order to ensure that regression testing process is
               cost-effective and practical. The techniques developed by Harrold, Rothermel, Rosenblum, Weyuker, Soffa
               and others have attempted to increase the cost-effectiveness of regression testing by intelligently
               selecting a subset of an entire regression test suite for execution. Recent approaches created by Elbaum,
               Malishevsky, Rothermel, Untch, Chu, and Harrold have tried to improve a regression test suite's rate of
               fault detection by prioritizing the execution of the tests. In this paper, we describe the conceptual
               foundation, design, and implementation of an approach that distributes the execution of regression test
               suites. We show that our technique can complement existing regression test selection and prioritization
               approaches. We also demonstrate that it can be used independently when existing approaches are not likely
               to increase regression testing efficiency. Finally, we describe the design and implementation of Joshua,
               an application that facilitates the distributed execution of regression test suites for Java-based
               software systems.}
}

@inproceedings{Kapfhammer2000,
  author     = {Gregory M. Kapfhammer and C.C. Michael and Jennifer Haddox and Ryan Colyer},
  title      = {An approach to identifying and understanding problematic COTS components},
  booktitle  = {Proceedings of the 2nd International Software Assurance and Certification Conference},
  year       = {2000},
  abstract   = {The usage of Commercial off the Shelf (COTS) components in software systems presents the possibility of
                temporal savings and efficiency increases. However, this temporal savings might come at the expense of
                system quality. When a system integrator relies upon COTS software, trust is placed in unknown,
                black-box components. We present a methodology that identifies problematic COTS components and then
                attempts to augment a system integrator's understanding of these components. Our technique uses software
                fault injection to expose COTS components to new failure scenarios. When these unique failure scenarios
                cause a COTS component to act in an unpredictable manner, our approach records the injected fault and
                the anomalous behavior. Next, we employ different machine learning techniques to build a representation
                of the anomalous behavior of the COTS component. These machine learning algorithms analyze the collected
                data, which describes the diverse conditions that cause a COTS component to behave unpredictably, and
                produce a comprehensive model of the combinations of input and component state that normally result in
                deviant behavior. A system integrator can inspect a graphical representation of this model in order to
                gain a better understanding of the anomalous COTS components. We believe our approach to isolating and
                understanding problematic COTS components will allow a system integrator to realize the temporal savings
                of reusable COTS software while also mitigating the associated risks.},
  nodownload = {true}
}

% CONFERENCE PAPERS }}}

% JOURNAL PAPERS {{{

@article{Althomali2021,
  author  = {Ibrahim Althomali and Gregory M. Kapfhammer and Phil McMinn},
  title   = {Automated visual classification of DOM-based presentation failure reports for responsive web pages},
  journal = {Software Testing, Verification and Reliability},
  volume  = {31},
  number  = {4},
  year    = {2021},
  abstract = {Since it is common for the users of a web page to access it through a wide variety of devices ---
              including desktops, laptops, tablets, and phones --- web developers rely on responsive web design (RWD)
              principles and frameworks to create sites that are useful on all devices. A correctly implemented
              responsive web page adjusts its layout according to the viewport width of the device in use, thereby
              ensuring its design suitably features the content. Since the use of complex RWD frameworks often leads to
              web pages with hard-to-detect responsive layout failures (RLFs), developers employ testing tools that
              generate reports of potential RLFs. Since testing tools for responsive web pages, like ReDeCheck, analyze
              a web page representation called the document object model (DOM), they may inadvertently flag concerns
              that are not human visible, thereby requiring developers to manually confirm and classify each potential
              RLF as a true positive (TP), false positive (FP), or non-observable issue (NOI) --- a process that is time
              consuming and error prone. The conference version of this paper presented VISER, a tool that automatically
              classified three types of RLFs reported by ReDeCheck. Since VISER was not designed to automatically
              confirm and classify two types of RLFs that ReDeCheck's DOM-based analysis could surface, this paper
              introduces VERVE, a tool that automatically classifies all RLF types reported by ReDeCheck. Along with
              manipulating the opacity of HTML elements in a web page, as does VISER, the VERVE tool also uses
              histogram-based image comparison to classify RLFs in web pages. Incorporating both the 25 web pages used
              in prior experiments and 20 new pages not previously considered, this paper’s empirical study reveals that
              VERVE's classification of all five types of RLFs frequently agrees with classifications produced manually
              by humans. The experiments also reveal that VERVE took on average about 4 seconds to classify any of the
              RLFs among the 469 reported by ReDeCheck. Since this paper demonstrates that classifying an RLF as a TP,
              FP, or NOI with VERVE, a publicly available tool, is less subjective and error-prone than the same manual
              process done by a human web developer, we argue that it is well-suited for supporting the testing of
              complex responsive web pages.},
  data     = {https://github.com/verve-tool/verve-tool.github.io},
  tool     = {https://github.com/verve-tool/verve}
}

@article{Walsh2020,
  author  = {Thomas A. Walsh and Gregory M. Kapfhammer and Phil McMinn},
  title   = {Automatically identifying potential regressions in the layout of responsive web pages},
  journal = {Software Testing, Verification and Reliability},
  volume  = {30},
  number  = {6},
  year    = {2020},
  abstract = {Providing a good user experience on the ever-increasing number and variety of devices being used to browse
              the web is a difficult, yet critical, task. With Responsive Web Design (RWD), front-end web developers
              design web pages so that they dynamically resize and rearrange content to best fit the dimensions of a
              device’s screen. However, when making code modifications to a responsive page, developers can easily
              introduce regressions from the correct layout that have detrimental effects at unpredictable screen sizes.
              For instance, the source code change that a developer makes to improve the layout at one screen size may
              obscure a page’s content at other sizes. Current approaches to testing are often insufficient because they rely
              on limited tools and error-prone manual inspections of a web page. As such, many unintended regressions
              in web page layout often go undetected and ultimately manifest in production web sites. To address the
              challenge of detecting regressions in responsive web pages, this paper presents an automated approach
              that extracts the responsive layout of two versions of a page and compares them, alerting developers to
              the differences in layout that they may wish to investigate further. We implemented the approach and
              empirically evaluated it on 15 real-world responsive web pages. Leveraging code mutations that a tool
              automatically injected into the pages as a systematic simulation of developer changes, the experiments
              show that the approach was highly effective. When compared with manual and automated baseline testing
              techniques, it detected 12.5% and 18.75% more injected changes, respectively. Along with identifying the
              best parameters for the method that extracts the responsive layout, the experiments show that the approach
              surpasses the baselines across changes that vary in their impact, but works particularly well for subtle,
              hard-to-detect mutants, showing the benefits of automatically identifying regressions in web page layout.},
  data     = {https://github.com/redecheck/jstvr-webpages},
  tool     = {https://github.com/redecheck/redecheck-tool}
}

@article{McMinn2019,
  author  = {McMinn, Phil and Wright, Chris and McCurdy, Colton and Kapfhammer, Gregory M.},
  title   = {Automatic detection and removal of ineffective mutants for the mutation analysis of relational database schemas},
  journal = {Transactions on Software Engineering},
  volume  = {45},
  number  = {5},
  year    = {2019},
  abstract = {Data is one of an organization's most valuable and strategic assets. Testing the relational database
              schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a
              means of estimating the fault-finding "strength" of a test suite. As with program mutation, however,
              relational database schema mutation results in many ineffective mutants that both degrade test suite
              quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of
              ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness
              with a series of key patterns evident in database schemas. On the basis of these, we introduce
              algorithms that automatically detect and remove ineffective mutants. In an experimental study
              involving the mutation analysis of 34 schemas used with three popular relational database management
              systems --- HyperSQL, PostgreSQL, and SQLite --- the results show that our algorithms can identify and
              discard large numbers of ineffective mutants that can account for up to 24% of mutants, leading to a
              change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve
              100% scores, indicating that they were capable of detecting and killing all non-equivalent mutants.
              The results also reveal that the execution cost of mutation analysis may be significantly reduced,
              especially with "heavyweight" DBMSs like PostgreSQL.},
  data     = {https://github.com/schemaanalyst/imdetect-replicate},
  tool     = {https://github.com/schemaanalyst/schemaanalyst}
}

@article{Lin2017,
  author  = {Lin, Chu-Ti and Tang, Kai-Wei and Wang, Jiun-Shiang and Kapfhammer, Gregory M.},
  title   = {Empirically evaluating greedy-based test suite reduction methods at different levels of test suite complexity},
  journal = {Science of Computer Programming},
  year    = {2017},
  abstract = {Test suite reduction is an important approach that decreases the cost of regression testing. A test
              suite reduction technique operates based on the relationship between the test cases in the
              regression test suite and the test requirements in the program under test. Thus, its effectiveness
              should be closely related to the complexity of a regression test suite --- the product of the number
              of test cases and the number of test requirements. Our previous work has shown that cost-aware
              techniques (i.e., the test suite reduction techniques that aim to decrease the regression test suite's
              execution cost) generally outperform the others in terms of decreasing the cost of running the
              regression test suite. However, the previous empirical studies that evaluated cost-aware
              techniques did not take into account test suite complexity. That is, prior experiments do not reveal
              if the cost-aware techniques scale and work effectively on test suites with more test cases and
              more test requirements. This means that prior experiments do not appropriately shed light on how
              well test suite reduction methods work with large programs or test suites. Therefore, this paper
              focuses on the Greedy-based techniques and empirically evaluates the additional Greedy and two
              cost-aware Greedy techniques --- at different levels of test suite complexity --- from various
              standpoints including the cost taken to run the regression test suite, the time taken to reduce the
              test suites, the total regression testing costs, the fault detection capability, the fault detection
              efficiency, and the common rates of the representative sets. To the best of our knowledge, none of
              the previous empirical studies classify a considerable number of test suites according to their
              complexity. Nor do any prior experiments evaluate the test suite reduction techniques, in terms of
              the aforementioned criteria, at different levels of test suite complexity. This paper represents the
              first such attempt to carry out this important task. Based on the empirical results, we confirm the
              strengths and weaknesses of the cost-aware techniques and develop insights into how the
              cost-aware techniques’ effectiveness varies as the test suite complexity increases.}
}

@article{McMinn2015,
  author  = {McMinn, Phil and Wright, Chris J. and Kapfhammer, Gregory M.},
  title   = {The effectiveness of test coverage criteria for relational
             database schema integrity constraints},
  journal = {Transactions on Software Engineering and Methodology},
  volume  = {25},
  number  = {1},
  year    = {2015},
  abstract = {Despite industry advice to the contrary, there has been little work that has sought to test that a
              relational database's schema has correctly specified integrity constraints. These critically important
              constraints ensure the coherence of data in a database, defending it from manipulations that could violate
              requirements such as "usernames must be unique" or "the host name cannot be missing or unknown". This
              paper is the first to propose coverage criteria, derived from logic coverage criteria, that establish
              different levels of testing for the formulation of integrity constraints in a database schema. These range
              from simple criteria that mandate the testing of successful and unsuccessful INSERT statements into tables
              to more advanced criteria that test the formulation of complex integrity constraints such as multi-column
              PRIMARY KEYS and arbitrary CHECK constraints. Due to different vendor interpretations of the structured
              query language (SQL) specification with regard to how integrity constraints should actually function in
              practice, our criteria crucially account for the underlying semantics of the database management system
              (DBMS).  After formally defining these coverage criteria and relating them in a subsumption hierarchy, we
              present two approaches to automatically generating tests that satisfy the criteria.  We then describe the
              results of an empirical study that uses mutation analysis to investigate the fault-finding capability of
              data generated when our coverage criteria are applied to a wide variety of relational schemas hosted by
              three well-known and representative DBMSs --- HyperSQL, PostgreSQL and SQLite.  In addition to revealing
              the complementary fault-finding capabilities of the presented criteria, the results show that mutation
              scores range from as low as just 12% of mutants being killed with the simplest of criteria to 96% with
              the most advanced.},
  tool      = {https://github.com/schemaanalyst/schemaanalyst}
}

@article{Lin2014,
  author  = {Lin, Chu-Ti and Tang, Kai-Wei and Kapfhammer, Gregory M.},
  title   = {Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests},
  journal = {Information and Software Technology},
  volume  = {56},
  number  = {10},
  year    = {2014},
  abstract = {<em>Context</em>: In software development and maintenance, a software system may frequently be updated to
              meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or
              modified functions, thus gradually increasing the test suite’s size. Test suite reduction techniques aim to
              decrease the cost of regression testing by removing the redundant test cases from the test suite and then
              obtaining a representative set of test cases that still yield a high level of code coverage. <p>
              <em>Objective</em>: Most of the existing reduction algorithms focus on decreasing the test suite’s size.
              Yet, the differences in execution costs among test cases are usually significant and it may take a lot of
              execution time to run a test suite consisting of a few long-running test cases. This paper presents and
              empirically evaluates cost-aware algorithms that can produce the representative sets with lower execution
              costs. <p> <em>Method</em>: We first use a cost-aware test case metric, called Irreplaceability, and its
              enhanced version, called EIrreplaceability, to evaluate the possibility that each test case can be replaced
              by others during test suite reduction. Furthermore, we construct a cost-aware framework that incorporates
              the concept of test irreplaceability into some well-known test suite reduction algorithms.  <p>
              <em>Results</em>: The effectiveness of the cost-aware framework is evaluated via the subject programs and
              test suites collected from the Software-artifact Infrastructure Repository --- frequently chosen benchmarks
              for experimentally evaluating test suite reduction methods. The empirical results reveal that the presented
              algorithms produce representative sets that normally incur a low cost to yield a high level of test
              coverage.  <p> <em>Conclusion</em>: The presented techniques indeed enhance the capability of the
              traditional reduction algorithms to reduce the execution cost of a test suite. Especially for the additional
              Greedy algorithm, the presented techniques decrease the costs of the representative sets by 8.10 -- 46.57%}
}

@article{Burdette2012,
  author  = {Philip F. Burdette and William F. Jones and Brian C. Blose and Gregory M. Kapfhammer},
  journal = {Performance Evaluation Review},
  title   = {An empirical comparison of Java remote communication primitives for intra-node data transmission},
  year    = {2012},
  month   = {April},
  volume  = {39},
  number  = {4},
  abstract = {This paper presents a benchmarking suite that measures the performance of using sockets and eXtensible
              Markup Language remote procedure calls (XML-RPC) to exchange intra-node messages between Java virtual
              machines (JVMs). The paper also reports on an empirical study comparing sockets and XML-RPC with response
              time measurements from timers that use both operating system tools and Java language instrumentation. By
              leveraging packet filters inside the GNU/Linux kernel, the benchmark suite also calculates network
              resource consumption. Moreover, the framework interprets the response time results in light of memory
              subsystem metrics characterizing the behavior of the JVM. The empirical findings indicate that sockets
              perform better when transmitting small to very large objects, while XML-RPC exhibits lower response time
              than sockets with extremely large bulk data transfers. The experiments reveal trade-offs in performance
              and thus represent the first step towards determining if Java remote communication primitives can support
              the efficient exchange of intra-node messages.}
}

@article{Fiedler2005,
  author   = {Daniel Fiedler and Kristen R. Walcott and Thomas Richardson and Gregory M. Kapfhammer and Ahmed Amer and Panos K. Chrysanthis},
  journal  = {Performance Evaluation Review},
  title    = {Towards the measurement of tuple space performance},
  year     = {2005},
  month    = {December},
  volume   = {33},
  number   = {3},
  abstract = {Many applications rely upon a tuple space within distributed system middleware to provide loosely coupled
              communication and service coordination. This paper describes an approach for measuring the throughput and
              response time of a tuple space when it handles concurrent local space interactions. Furthermore, it
              discusses a technique that populates a tuple space with tuples before the execution of a benchmark in
              order to age the tuple space and provide a worst-case measurement of space performance. We apply the tuple
              space benchmarking and aging methods to the measurement of the performance of a JavaSpace, a current
              example of a tuple space that integrates with the Jini network technology. The experiment results indicate
              that: (i) the JavaSpace exhibits limited scalability as the number of concurrent interactions from local
              space clients increases, (ii) the aging technique can operate with acceptable time overhead, and (iii) the
              aging technique does ensure that the results from benchmarking capture the worst-case performance of a
              tuple space.}
}

% JOURNAL PAPERS }}}

% VOLUMES EDITED {{{

@article{Chan2011,
  author     = {Wing-Kwong Chan and Christof J. Budnik and Gregory M. Kapfhammer and Hong Zhu},
  journal    = {Software Quality Journal},
  title      = {Special section: Exploring the boundaries of software test automation},
  year       = {2011},
  volume     = {19},
  number     = {3},
  keywords   = {edit},
  nodownload = {true}
}

@article{Bottaci2010,
  author     = {Leonardo Bottaci and Gregory M. Kapfhammer and Marc Roper},
  journal    = {Information and Software Technology},
  title      = {Special section of selected papers from the Testing: {Academic} and {Industrial} {Conference} -- {Practice} and {Research} {Techniques}},
  year       = {2010},
  volume     = {52},
  number     = {5},
  keywords   = {edit},
  nodownload = {true}
}

@article{Bottaci2010a,
  author   = {Leonardo Bottaci and Gregory M. Kapfhammer and Neil Walkinshaw},
  journal  = {Journal of Systems and Software},
  title    = {Special section of selected papers from the Testing: {Academic} and {Industrial} {Conference} -- {Practice} and {Research} {Techniques}},
  year     = {2010},
  volume   = {83},
  number   = {12},
  keywords = {edit},
  nodownload = {true}
}

@article{Laplante2010,
  author     = {Philip Laplante and Fevzi Belli and Jerry Gao and Gregory M. Kapfhammer and Keith Miller and W. Eric Wong},
  journal    = {Advances in Software Engineering},
  title      = {Special issue: Software test automation},
  year       = {2010},
  keywords   = {edit},
  nodownload = {true}
}

% VOLUMES EDITED }}}

% PRESENTATIONS {{{

@misc{Kapfhammer2021,
  author       = {Gregory M. Kapfhammer},
  title        = {Committing to writing good commit messages: Supporting the creation of human- and machine-readable commit messages with Python},
  howpublished = {PyCon Lightning Talk Session},
  year         = {2021},
  addendum     = {Joint work with Teona Bagashvili},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/committing-to-writing-good-commit-messages-supporting-the-creation-of-human-and-machine-readable-commit-messages-with-python}
}

@misc{Kapfhammer2021a,
  author       = {Gregory M. Kapfhammer},
  title        = {Type annotations in Python: Terribly intimidating or tremendously informative?},
  howpublished = {PyOhio},
  year         = {2021},
  addendum     = {Joint work with Madelyn Kapfhammer},
  talk         = {true},
  nodownload   = {true}
}

@misc{Kapfhammer2021b,
  author       = {Gregory M. Kapfhammer},
  title        = {TaDa it's magic: Predicting the performance of functions through automated doubling experiments},
  howpublished = {Code PaLOUsa},
  year         = {2021},
  addendum     = {Joint work with Lancaster Wu and Enpu You},
  talk         = {true},
  nodownload   = {true}
}

@misc{Kapfhammer2021c,
  author       = {Gregory M. Kapfhammer},
  title        = {Great on their own, even better together: Application development with Python, Typer, and Poetry},
  howpublished = {Code PaLOUsa},
  year         = {2021},
  addendum     = {Joint work with Teona Bagashvili},
  talk         = {true},
  nodownload   = {true}
}

@misc{Kapfhammer2019,
  author       = {Gregory M. Kapfhammer},
  title        = {Using Python and GitHub for team formation and assessment},
  howpublished = {PyCon Poster Symposium},
  year         = {2019},
  addendum     = {Joint work with Jahlia Finney, Mohammad Khan, and Carson Quigley},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/using-python-and-github-for-team-formation-and-assessment}
}

@misc{Kapfhammer2019a,
  author       = {Gregory M. Kapfhammer},
  title        = {Automatic detection of pseudo-tested methods using Python and Pytest},
  howpublished = {PyCon Poster Symposium},
  year         = {2019},
  addendum     = {Joint work with Nicholas Tocci},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/automatic-detection-of-pseudo-tested-methods-using-python-and-pytest}
}

@misc{Kapfhammer2019b,
  author       = {Gregory M. Kapfhammer},
  title        = {Find your feature fit: How to pick a text editor for Python programming},
  howpublished = {PyOhio},
  year         = {2019},
  addendum     = {Joint work with Madelyn Kapfhammer},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/find-your-feature-fit-how-to-pick-a-text-editor-for-python-programming}
}

@misc{Kapfhammer2018,
  author       = {Gregory M. Kapfhammer},
  title        = {Using GitHub, Travis CI, and Python to introduce collaborative software development},
  howpublished = {PyCon Education Summit},
  year         = {2018},
  addendum     = {Joint work with Rowan Castellanos, Saejin Mahlau-Heinert, and Nicholas Tocci},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/using-github-travis-ci-and-python-to-introduce-collaborative-software-development}
}

@misc{Kapfhammer2018a,
  author       = {Gregory M. Kapfhammer},
  title        = {A hands-on guide to teaching programming with GitHub, Travis CI, and Python},
  howpublished = {PyOhio},
  year         = {2018},
  addendum     = {Joint work with Rowan Castellanos, Saejin Mahlau-Heinert, and Nicholas Tocci},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/a-hands-on-guide-to-teaching-programming-with-github-travis-ci-and-python}
}

@misc{Kapfhammer2018b,
  author       = {Gregory M. Kapfhammer},
  title        = {Using Python, Travis CI, and GitHub to effectively teach programming},
  howpublished = {PyGotham},
  year         = {2018},
  addendum     = {Joint work with Rowan Castellanos, Saejin Mahlau-Heinert, and Nicholas Tocci},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/using-python-travis-ci-and-github-to-effectively-teach-programming}
}

@misc{Kapfhammer2015a,
  author       = {Gregory M. Kapfhammer},
  title        = {Is big data a big deal? Not without correct software!},
  howpublished = {27th International Conference on Software Engineering and Knowledge Engineering -- Panel},
  year         = {2015},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/is-big-data-a-big-deal-not-without-correct-software}
}

@misc{Kapfhammer2015,
  author       = {Gregory M. Kapfhammer},
  title        = {Searching for the best tests: An introduction to automated software testing with search-based techniques},
  howpublished = {Allegheny College},
  year         = {2015},
  talk         = {true},
  presentation = {https://speakerdeck.com/gkapfham/searching-for-the-best-tests}
  }

@misc{Kapfhammer2012,
  author       = {Gregory M. Kapfhammer},
  title        = {Using dynamic invariant detection to support the testing and analysis of database applications},
  howpublished = {University of Ulm},
  year         = {2012},
  addendum     = {Joint work with Jake Cobb, James A.\ Jones, Mary Jean Harrold, and Jonathan Miller Kauffman},
  presentation = {https://speakerdeck.com/gkapfham/using-dynamic-invariant-detection-to-support-the-testing-and-analysis-of-database-applications},
  talk         = {true}
}

@misc{Kapfhammer2012pa,
  author       = {Gregory M. Kapfhammer},
  title        = {Regression testing techniques for relational database applications},
  howpublished = {University of Ulm},
  year         = {2012},
  addendum     = {Joint work with Jonathan Miller Kauffman and Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/regression-testing-techniques-for-relational-database-applications},
  talk         = {true}
}

@misc{Kapfhammer2012pb,
  author       = {Gregory M. Kapfhammer},
  title        = {Practical techniques for improving the efficiency and usability of mutation analysis for Java programs},
  howpublished = {University of Sheffield},
  year         = {2012},
  addendum     = {Joint work with Ren\'{e} Just and Franz Schweiggert},
  presentation = {https://speakerdeck.com/gkapfham/practical-techniques-for-improving-the-efficiency-and-usability-of-mutation-analysis-for-java-programs},
  talk         = {true}
}

@misc{Kapfhammer2012pc,
  author = {Gregory M. Kapfhammer},
  title  = {Efficient and effective mutation testing: Supporting the implementation of quality software by
            purposefully inserting defects},
  howpublished = {University of Delhi},
  year         = {2012},
  addendum     = {Joint work with Ren\'{e} Just and Franz Schweiggert},
  presentation = {https://speakerdeck.com/gkapfham/efficient-and-effective-mutation-testing-supporting-the-implementation-of-quality-software-by-purposefully-inserting-defects},
  talk         = {true}
}

@misc{Kapfhammer2012pd,
  author = {Gregory M. Kapfhammer},
  title  = {Software quality improvement through repeated test execution: An exploration of the present and
            future of regression testing},
  howpublished = {University of Delhi},
  year         = {2012},
  addendum     = {Joint work with Jonathan Miller Kauffman},
  presentation = {https://speakerdeck.com/gkapfham/software-quality-improvement-through-repeated-test-execution-an-exploration-of-the-present-and-future-of-regression-testing},
  talk         = {true}
}

@misc{Kapfhammer2011pa,
  author       = {Gregory M. Kapfhammer},
  title        = {Regression testing: Theoretical underpinnings, practical techniques, and empirical insights},
  howpublished = {15th International Conference on Software Engineering and Applications -- Tutorial},
  year         = {2011},
  addendum     = {Joint work with Jonathan Miller Kauffman},
  presentation = {https://speakerdeck.com/gkapfham/regression-testing-theoretical-underpinnings-practical-techniques-and-empirical-insights},
  talk         = {true}
}

@misc{Kapfhammer2011pb,
  author       = {Gregory M. Kapfhammer},
  title        = {MAJOR: An efficient technique for mutation analysis in a Java compiler},
  howpublished = {4th International Conference on Software Testing, Verification and Validation -- Poster},
  year         = {2011},
  addendum     = {Joint work with Ren\'{e} Just},
  talk         = {true}
}

@misc{Kapfhammer2009pa,
  author = {Gregory M. Kapfhammer},
  title  = {Practical suggestions for improving and empirically studying greedy test suite reduction and
            prioritization methods},
  howpublished = {Nanjing University},
  year         = {2009},
  addendum     = {Joint work with Joshua J. Geiger, G. Elisabeta Marai, Manos Renieris, and Adam M. Smith},
  presentation = {https://speakerdeck.com/gkapfham/practical-suggestions-for-improving-and-empirically-studying-greedy-test-suite-reduction-and-prioritization-methods},
  talk         = {true}
}

@misc{Kapfhammer2009pb,
  author       = {Gregory M. Kapfhammer},
  title        = {The measured performance of declarative approaches to finding data in unstructured heaps},
  howpublished = {Westminster College},
  year         = {2009},
  addendum     = {Joint work with William F. Jones},
  presentation = {https://speakerdeck.com/gkapfham/the-measured-performance-of-declarative-approaches-to-finding-data-in-unstructured-heaps},
  talk         = {true}
}

@misc{Kapfhammer2008pa,
  author       = {Gregory M. Kapfhammer},
  title        = {The theory and practice of software testing: Can we test it? Yes we can!},
  howpublished = {Symcon Global Technologies India},
  year         = {2008},
  addendum     = {Joint work with Suvarshi Bhadra, Adam M. Smith, Mary Lou Soffa, and Kristen R. Walcott},
  presentation = {https://speakerdeck.com/gkapfham/the-theory-and-practice-of-software-testing-can-we-test-it-yes-we-can},
  talk         = {true}
}

@misc{Kapfhammer2008pb,
  author       = {Gregory M. Kapfhammer},
  title        = {Set covers, knapsacks, and regression testing techniques},
  howpublished = {Madras Christian College},
  year         = {2008},
  addendum     = {Joint work with Suvarshi Bhadra, Adam M. Smith, Mary Lou Soffa, and Kristen R. Walcott},
  presentation = {https://speakerdeck.com/gkapfham/set-covers-knapsacks-and-regression-testing-techniques},
  talk         = {true}
}

@misc{Kapfhammer2008pc,
  author       = {Gregory M. Kapfhammer},
  title        = {Using synthetic coverage information to evaluate test suite prioritizers},
  howpublished = {Chennai Mathematical Institute},
  year         = {2008},
  addendum     = {Joint work with Suvarshi Bhadra and Yuting Zhang},
  presentation = {https://speakerdeck.com/gkapfham/using-synthetic-coverage-information-to-evaluate-test-suite-prioritizers},
  talk         = {true}
}

@misc{Kapfhammer2008pd,
  author       = {Gregory M. Kapfhammer},
  title        = {Can search-based prioritizers improve the coverage effectiveness of regression test suites?},
  howpublished = {King's College London},
  year         = {2008},
  addendum     = {Joint work with Alexander P. Conrad},
  nodownload   = {true},
  talk         = {true}
}

@misc{Kapfhammer2007p,
  author       = {Gregory M. Kapfhammer},
  title        = {Automatic program instrumentation to the rescue!},
  howpublished = {Allegheny College},
  year         = {2007},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/automatic-program-instrumentation-to-the-rescue},
  talk         = {true}
}

@misc{Kapfhammer2007pa,
  author       = {Gregory M. Kapfhammer},
  title        = {The measured performance of communication and serialization primitives},
  howpublished = {NITLE Workshop on Parallel and Cluster Computing},
  year         = {2007},
  addendum     = {Joint work with Brian C. Blose and Philip P. Burdette},
  presentation = {https://speakerdeck.com/gkapfham/the-measured-performance-of-communication-and-serialization-primitives},
  talk         = {true}
}

@misc{Kapfhammer2007pb,
  author       = {Gregory M. Kapfhammer},
  title        = {The measured performance of database-aware test coverage monitoring},
  howpublished = {University of Pittsburgh},
  year         = {2007},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/the-measured-performance-of-database-aware-test-coverage-monitoring},
  talk         = {true}
}

@misc{Kapfhammer2007pc,
  author       = {Gregory M. Kapfhammer},
  title        = {Exploring approaches to time-aware test suite prioritization},
  howpublished = {Microsoft Research Asia},
  year         = {2007},
  addendum     = {Joint work with Mary Lou Soffa and Kristen R. Walcott},
  presentation = {https://speakerdeck.com/gkapfham/exploring-approaches-to-time-aware-test-suite-prioritization},
  talk         = {true}
}

@misc{Kapfhammer2007pe,
  author       = {Gregory M. Kapfhammer},
  title        = {Towards regression testing for database applications},
  howpublished = {Analysis, Slicing, and Transformation Research Network},
  year         = {2007},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/towards-regression-testing-for-database-applications},
  talk         = {true}
}

@misc{Kapfhammer2006pd,
  author       = {Gregory M. Kapfhammer},
  title        = {Measuring the performance of an XML-based communication primitive},
  howpublished = {Allegheny College},
  year         = {2006},
  addendum     = {Joint work with Brian C. Blose},
  presentation = {https://speakerdeck.com/gkapfham/measuring-the-performance-of-an-xml-based-communication-primitive},
  talk         = {true}
}

@misc{Kapfhammer2005p,
  author       = {Gregory M. Kapfhammer},
  title        = {SETTLE: A tuple space benchmarking and testing framework},
  howpublished = {9th Jini Community Meeting},
  year         = {2005},
  addendum     = {Joint work with Ahmed Amer, Panos K. Chrysanthis, Daniel Fiedler, Thomas Richardson, and Kristen R. Walcott},
  presentation = {https://speakerdeck.com/gkapfham/settle-a-tuple-space-benchmarking-and-testing-framework},
  talk         = {true}
}

@misc{Kapfhammer2005pa,
  author       = {Gregory M. Kapfhammer},
  title        = {Further experience with teaching distributed systems to undergraduates},
  howpublished = {9th Jini Community Meeting},
  year         = {2005},
  presentation = {https://speakerdeck.com/gkapfham/further-experience-with-teaching-distributed-systems-to-undergraduates},
  talk         = {true}
}

@misc{Kapfhammer2005pb,
  author       = {Gregory M. Kapfhammer},
  title        = {A test adequacy infrastructure with database interaction awareness},
  howpublished = {University of California, Santa Barbara},
  year         = {2005},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/a-test-adequacy-infrastructure-with-database-interaction-awareness},
  talk         = {true}
}

@misc{Kapfhammer2005pc,
  author       = {Gregory M. Kapfhammer},
  title        = {Empirical evaluation of an approach to resource-constrained test suite execution},
  howpublished = {University of Pittsburgh},
  year         = {2005},
  addendum     = {Joint work with Daniel Moss\'{e} and Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/empirical-evaluation-of-an-approach-to-resource-constrained-test-suite-execution},
  talk         = {true}
}

@misc{Kapfhammer2004p,
  author       = {Gregory M. Kapfhammer},
  title        = {Testing database-driven applications: Challenges and solutions},
  howpublished = {IBM T.J. Watson Research Center},
  year         = {2004},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/testing-database-driven-applications-challenges-and-solutions},
  talk         = {true}
}

@misc{Kapfhammer2004pa,
  author       = {Gregory M. Kapfhammer},
  title        = {A primer on testing database-driven applications},
  howpublished = {Allegheny College},
  year         = {2004},
  addendum     = {Joint work with Mary Lou Soffa},
  presentation = {https://speakerdeck.com/gkapfham/a-primer-on-testing-database-driven-applications},
  talk         = {true}
}

@misc{Kapfhammer2002pa,
  author       = {Gregory M. Kapfhammer},
  title        = {Towards a JavaSpace supported by a relational database},
  howpublished = {6th Jini Community Meeting},
  year         = {2002},
  addendum     = {Joint work with Geoffrey C. Arnold and Robert S. Roos},
  nodownload   = {true},
  talk         = {true}
}

@misc{Kapfhammer2002pb,
  author       = {Gregory M. Kapfhammer},
  title        = {Building distributed genetic algorithms with the Jini network technology},
  howpublished = {6th Jini Community Meeting},
  year         = {2002},
  addendum     = {Joint work with Robert S. Roos and Brian Zorman},
  presentation = {https://speakerdeck.com/gkapfham/building-distributed-genetic-algorithms-with-the-jini-network-technology},
  talk         = {true}
}

@misc{Kapfhammer2002pc,
  author       = {Gregory M. Kapfhammer},
  title        = {Improving the Jini out-of-box experience: Lessons learned and solutions provided},
  howpublished = {6th Jini Community Meeting},
  year         = {2002},
  addendum     = {Joint work with Geoffrey C. Arnold, Charles R. DiVittorio, Brian Hykes,
                  Mehrnoush Moussavi-Aghdam, and James E. Tomayko},
  nodownload   = {true},
  talk         = {true}
}

@misc{Kapfhammer2002pd,
  author       = {Gregory M. Kapfhammer},
  title        = {Teaching distributed systems to undergraduates: An experience report},
  howpublished = {6th Jini Community Meeting},
  year         = {2002},
  presentation = {https://speakerdeck.com/gkapfham/teaching-distributed-systems-to-undergraduates-an-experience-report},
  talk         = {true}
}

@misc{Kapfhammer2000pe,
  author       = {Gregory M. Kapfhammer},
  title        = {An automated wrapping system for Java components and Jini services},
  howpublished = {17th International Conference on Testing Computer Software},
  year         = {2000},
  nodownload   = {true},
  talk         = {true}
}

% PRESENTATIONS }}}
